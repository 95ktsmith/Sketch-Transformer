{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cpySketchformer-F.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BeGIZe8AvX7G",
        "0QnZ_WeSwnwB",
        "twNEKMkAImxD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeGIZe8AvX7G"
      },
      "source": [
        "# Downloads and model classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYNTzE8AvWiD",
        "outputId": "5b1429f1-37f9-4b07-aafb-a05c3762d874"
      },
      "source": [
        "!pip install svgwrite\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.1-py3-none-any.whl (66 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 20 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 30 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 66 kB 3.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite\n",
            "Successfully installed svgwrite-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axEzp6a1vm5k"
      },
      "source": [
        "def positional_encoding(max_seq_len, dm):\n",
        "    \"\"\"\n",
        "    Calculates the positional encoding for a transformer\n",
        "    max_seq_len: integer representing the maximum sequence length\n",
        "    dm: integer representing the model depth\n",
        "    Returns: numpy.ndarray of shape (max_seq_len, dm) containing the positional\n",
        "             encoding vectors\n",
        "    \"\"\"\n",
        "    PE = np.zeros((max_seq_len, dm))\n",
        "    for row in range(max_seq_len):\n",
        "        for col in range(0, dm, 2):\n",
        "            PE[row, col] = np.sin(row / (10000 ** (col / dm)))\n",
        "            PE[row, col + 1] = np.cos(row / (10000 ** (col / dm)))\n",
        "    return PE\n",
        "\n",
        "\n",
        "def sdp_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Q: tensor with shape (..., seq_len_q, dk) containing the query matrix\n",
        "    K: tensor with shape (..., seq_len_v, dk) containing the key matrix\n",
        "    V: tensor with shape (..., seq_len_v, dv) containing the value matrix\n",
        "    mask: tensor that can be broadcast into (..., seq_len_q, seq_len_v)\n",
        "          containing the optional maask, or defaulted to None\n",
        "    The Preceding dimensions of Q, K, and V are the same\n",
        "    Returns: output, weights\n",
        "             output: tensor with shape (..., seq_len_q, dv) containing the dot\n",
        "                     product attention\n",
        "             weights: tensor with shape (..., seq_len_q, seq_len_v) containing\n",
        "                      the attention weights\n",
        "    \"\"\"\n",
        "    # Matmul Q and K\n",
        "    QK = tf.matmul(Q, K, transpose_b=True)\n",
        "\n",
        "    # Scale the dot product\n",
        "    dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "    scaled = QK / tf.math.sqrt(dk)\n",
        "\n",
        "    # Add mask if not None\n",
        "    if mask is not None:\n",
        "        scaled += mask * -1e9\n",
        "\n",
        "    # Pass scaled attention through softmax activation\n",
        "    weights = tf.nn.softmax(scaled, axis=-1)\n",
        "\n",
        "    # Matmul by value matrix for output\n",
        "    output = tf.matmul(weights, V)\n",
        "\n",
        "    return output, weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class to perform multi head attention\n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h):\n",
        "        \"\"\"\n",
        "        dm: integer representing the model dimensionality\n",
        "        h: integer representing the number of heads\n",
        "        dm is divisible by h\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.h = h\n",
        "        self.dm = dm\n",
        "        self.depth = dm // self.h\n",
        "        self.Wq = tf.keras.layers.Dense(dm)\n",
        "        self.Wk = tf.keras.layers.Dense(dm)\n",
        "        self.Wv = tf.keras.layers.Dense(dm)\n",
        "        self.linear = tf.keras.layers.Dense(dm)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Splits the last dimension of tensor x into (h, depth)\n",
        "        Transpose the result such that the shape is\n",
        "        (batch_size, h, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.h, self.depth))\n",
        "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        return x\n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        \"\"\"\n",
        "        Q: tensor with shape (..., seq_len_q, dk) containing the query matrix\n",
        "        K: tensor with shape (..., seq_len_v, dk) containing the key matrix\n",
        "        V: tensor with shape (..., seq_len_v, dv) containing the value matrix\n",
        "        mask: always None\n",
        "        The Preceding dimensions of Q, K, and V are the same\n",
        "        Returns: output, weights\n",
        "                 output: tensor with shape (..., seq_len_q, dv) containing the\n",
        "                         dot product attention\n",
        "                 weights: tensor with shape (..., seq_len_q, seq_len_v)\n",
        "                          containing the attention weights\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(Q)[0]\n",
        "\n",
        "        # Generate query, key, and value matrices\n",
        "        Q = self.Wq(Q)\n",
        "        K = self.Wk(K)\n",
        "        V = self.Wv(V)\n",
        "\n",
        "        # Split between heads\n",
        "        Q = self.split_heads(Q, batch_size)\n",
        "        K = self.split_heads(K, batch_size)\n",
        "        V = self.split_heads(V, batch_size)\n",
        "\n",
        "        # Scaled Dot Product Attention\n",
        "        attention, weights = sdp_attention(Q, K, V, mask)\n",
        "\n",
        "        # Refit to pass through linear layer\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        attention = tf.reshape(attention, (batch_size, -1, self.dm))\n",
        "        output = self.linear(attention)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "class EncoderBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class representation of an encoder block for a transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h, hidden, drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        dm: Dimensionality of the model\n",
        "        h: Number of heads\n",
        "        hidden: Number of hidden units in the fully connected layer\n",
        "        drop_rate: Dropout rate\n",
        "        \"\"\"\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.mha = MultiHeadAttention(dm, h)\n",
        "        self.dense_hidden = tf.keras.layers.Dense(\n",
        "            units=hidden,\n",
        "            activation='relu'\n",
        "        )\n",
        "        self.dense_output = tf.keras.layers.Dense(dm)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch, input_seq_len, dm) containing the input to\n",
        "           the encoder block\n",
        "        training: boolean to determine if the model is training\n",
        "        mask: the mask to be applied for multi head attention\n",
        "        Returns: tensor of the shape (batch, input_seq_len, dm) containing the\n",
        "                 block's output\n",
        "        \"\"\"\n",
        "        # Pass through multi head attention and dropout layers\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "\n",
        "        # Add and normalize\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        # Feed through dense layers and dropout layer\n",
        "        dense_output = self.dense_hidden(out1)\n",
        "        dense_output = self.dense_output(dense_output)\n",
        "        dense_output = self.dropout2(dense_output, training=training)\n",
        "\n",
        "        # Add and normalize\n",
        "        out2 = self.layernorm2(out1 + dense_output)\n",
        "\n",
        "        return out2\n",
        "\n",
        "\n",
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class representation of a decoder block for a transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h, hidden, drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        dm: Dimensionality of the model\n",
        "        h: Number of heads\n",
        "        hidden: Number of hidden units in the fully connected layer\n",
        "        drop_rate: Dropout rate\n",
        "        \"\"\"\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(dm, h)\n",
        "        self.mha2 = MultiHeadAttention(dm, h)\n",
        "        self.dense_hidden = tf.keras.layers.Dense(\n",
        "            units=hidden,\n",
        "            activation='relu'\n",
        "        )\n",
        "        self.dense_output = tf.keras.layers.Dense(units=dm)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, encoder_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch, target_seq_len, dm)containing the input to\n",
        "           the decoder block\n",
        "        encoder_output: tensor of shape (batch, input_seq_len, dm)containing\n",
        "                        the output of the encoder\n",
        "        training: boolean to determine if the model is training\n",
        "        look_ahead_mask: mask to be applied to the first multi head attention\n",
        "                         layer\n",
        "        padding_mask: mask to be applied to the second multi head attention\n",
        "                      layer\n",
        "        Returns: tensor of shape (batch, target_seq_len, dm) containing the\n",
        "                 block's output\n",
        "        \"\"\"\n",
        "        # Pass through first round of MHA and dropout layers\n",
        "        attn_out, _ = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn_out = self.dropout1(attn_out, training=training)\n",
        "\n",
        "        # Add and normalize\n",
        "        out1 = self.layernorm1(x + attn_out)\n",
        "\n",
        "        # Pass trough second round of MHA and dropout layers\n",
        "        attn_out, _ = self.mha2(\n",
        "            out1,\n",
        "            encoder_output,\n",
        "            encoder_output,\n",
        "            padding_mask\n",
        "        )\n",
        "        attn_out = self.dropout2(attn_out, training=training)\n",
        "\n",
        "        # Add and normalize\n",
        "        out2 = self.layernorm2(out1 + attn_out)\n",
        "\n",
        "        # Pass through dense layers and dropout layer\n",
        "        dense_output = self.dense_hidden(out2)\n",
        "        dense_output = self.dense_output(dense_output)\n",
        "        dense_output = self.dropout3(dense_output, training=training)\n",
        "\n",
        "        # Add and normalize\n",
        "        out3 = self.layernorm3(out2 + dense_output)\n",
        "\n",
        "        return out3\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class representation of an encoder for a transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, N, dm, h, hidden, max_seq_len,\n",
        "                 drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        N: Number of blocks in the encoder\n",
        "        dm: Dimensionality of the model\n",
        "        h: Number of heads\n",
        "        hidden: Number of hidden units in the fully connected layer\n",
        "        input_vocab: Size of the input vocabulary\n",
        "        max_seq_len: Maximum sequence length possible\n",
        "        drop_rate: Dropout rate\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.N = N\n",
        "        self.dm = dm\n",
        "        self.projecting = tf.keras.layers.Dense(dm)\n",
        "        # self.embedding = tf.keras.layers.Embedding(input_vocab, dm)\n",
        "        self.positional_encoding = positional_encoding(max_seq_len, dm)\n",
        "        self.blocks = []\n",
        "        for n in range(N):\n",
        "            self.blocks.append(EncoderBlock(dm, h, hidden, drop_rate))\n",
        "        self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch, input_seq_len, dm) containing the input to\n",
        "           the encoder\n",
        "        training: boolean to determine if the model is training\n",
        "        mask: mask to the applied for multi head attention\n",
        "        Returns: tensor of shape (batch, input_seq_len, dm) containing the\n",
        "                 encoder output\n",
        "        \"\"\"\n",
        "        seq_len = int(x.shape[1])\n",
        "\n",
        "        # Pass input through embedding layer\n",
        "        # x = self.embedding(x)\n",
        "\n",
        "        # Pass input through projecting layer\n",
        "        x = self.projecting(x)\n",
        "\n",
        "        # Add positional encoding, pass through dropout layer\n",
        "        x *= tf.math.sqrt(tf.cast(self.dm, 'float32'))\n",
        "        x += self.positional_encoding[:seq_len]\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # Pass through each encoding block\n",
        "        for block in self.blocks:\n",
        "            x = block(x, training, mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class representation of a decoder for a transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, N, dm, h, hidden, max_seq_len,\n",
        "                 drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        dm - the dimensionality of the model\n",
        "        h - the number of heads\n",
        "        hidden - the number of hidden units in the fully connected layer\n",
        "        target_vocab - deprecated\n",
        "        max_seq_len - the maximum sequence length possible\n",
        "        drop_rate - the dropout rate\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        self.N = N\n",
        "        self.dm = dm\n",
        "        # self.embedding = tf.keras.layers.Embedding(target_vocab, dm)\n",
        "        self.projecting = tf.keras.layers.Dense(dm)\n",
        "        self.positional_encoding = positional_encoding(max_seq_len, dm)\n",
        "        self.blocks = []\n",
        "        for n in range(N):\n",
        "            self.blocks.append(DecoderBlock(dm, h, hidden, drop_rate))\n",
        "        self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, encoder_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        x - a tensor of shape (batch, target_seq_len, dm) containing the input\n",
        "            to the decoder\n",
        "        encoder_output - a tensor of shape (batch, input_seq_len, dm)\n",
        "            containing the output of the encoder\n",
        "        training - a boolean to determine if the model is training\n",
        "        look_ahead_mask - the mask to be applied to the first multi head\n",
        "            attention layer\n",
        "        padding_mask - the mask to be applied to the second multi head\n",
        "            attention layer\n",
        "        Returns: a tensor of shape (batch, target_seq_len, dm) containing the\n",
        "            decoder output\n",
        "        \"\"\"\n",
        "        seq_len = int(x.shape[1])\n",
        "\n",
        "        # Pass through embedding layer\n",
        "        # x = self.embedding(x)\n",
        "\n",
        "        # Replace embedding with projecting\n",
        "        x = self.projecting(x)\n",
        "\n",
        "        # Add positional encoding and pass through dropout layer\n",
        "        x *= tf.math.sqrt(tf.cast(self.dm, 'float32'))\n",
        "        x += self.positional_encoding[:seq_len]\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # Pass through each decoder block\n",
        "        for block in self.blocks:\n",
        "            x = block(\n",
        "                x,\n",
        "                encoder_output,\n",
        "                training,\n",
        "                look_ahead_mask,\n",
        "                padding_mask\n",
        "            )\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Class representation of a transformer network\n",
        "    \"\"\"\n",
        "    def __init__(self, N, dm, h, hidden,\n",
        "                 max_seq_input, max_seq_target, drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        N - the number of blocks in the encoder and decoder\n",
        "        dm - the dimensionality of the model\n",
        "        h - the number of heads\n",
        "        hidden - the number of hidden units in the fully connected layers\n",
        "        input_vocab - deprecated\n",
        "        target_vocab - deprecated\n",
        "        max_seq_input - the maximum sequence length possible for the input\n",
        "        max_seq_target - the maximum sequence length possible for the target\n",
        "        drop_rate - the dropout rate\n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(\n",
        "            N,\n",
        "            dm,\n",
        "            h,\n",
        "            hidden,\n",
        "            max_seq_input,\n",
        "            drop_rate\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            N,\n",
        "            dm,\n",
        "            h,\n",
        "            hidden,\n",
        "            max_seq_target,\n",
        "            drop_rate\n",
        "        )\n",
        "        self.linear = tf.keras.layers.Dense(units=5)\n",
        "\n",
        "    def call(self, inputs, target, training, encoder_mask, look_ahead_mask,\n",
        "             decoder_mask):\n",
        "        \"\"\"\n",
        "        inputs - a tensor of shape (batch, input_seq_len)containing the inputs\n",
        "        target - a tensor of shape (batch, target_seq_len)containing the target\n",
        "        training - a boolean to determine if the model is training\n",
        "        encoder_mask - the padding mask to be applied to the encoder\n",
        "        look_ahead_mask - the look ahead mask to be applied to the decoder\n",
        "        decoder_mask - the padding mask to be applied to the decoder\n",
        "        Returns: a tensor of shape (batch, target_seq_len, target_vocab)\n",
        "            containing the transformer output\n",
        "        \"\"\"\n",
        "        encoder_output = self.encoder(inputs, training, encoder_mask)\n",
        "        decoder_output = self.decoder(\n",
        "            target,\n",
        "            encoder_output,\n",
        "            training,\n",
        "            look_ahead_mask,\n",
        "            decoder_mask\n",
        "        )\n",
        "        output = self.linear(decoder_output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QnZ_WeSwnwB"
      },
      "source": [
        "# dataset class and masking f'n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aceg1zkXwm_S"
      },
      "source": [
        "# Function from utils.py for sketch-rnn in the Magenta github repository\n",
        "# at https://github.com/magenta/magenta/tree/main/magenta/models/sketch_rnn\n",
        "def to_big_strokes(stroke, max_len=250):\n",
        "  \"\"\"Converts from stroke-3 to stroke-5 format and pads to given length.\"\"\"\n",
        "  # (But does not insert special start token).\n",
        "\n",
        "  result = np.zeros((max_len, 5), dtype=float)\n",
        "  l = len(stroke)\n",
        "  assert l <= max_len\n",
        "  result[0:l, 0:2] = stroke[:, 0:2]\n",
        "  result[0:l, 3] = stroke[:, 2]\n",
        "  result[0:l, 2] = 1 - result[0:l, 3]\n",
        "  result[l:, 4] = 1\n",
        "  return result\n",
        "\n",
        "# Tokenizing isn't being used anymore but these functions are being kept\n",
        "# until we're absolutely certain we don't need them\n",
        "\n",
        "def strokes_to_tokens(strokes):\n",
        "    \"\"\"\n",
        "    strokes is a 2d numpy array of stroke-5 vectors to be converted into tokens\n",
        "    Returns a list of the tokenized vectors\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    for stroke in strokes:\n",
        "        token = (stroke[0] + 255) * 511\n",
        "        token += (stroke[1] + 255)\n",
        "        token += np.sum(np.array([100, 500000, 1000000]) * stroke[2:])\n",
        "        tokens.append(token)\n",
        "    return np.asarray(tokens)\n",
        "\n",
        "def tokens_to_strokes(tokens):\n",
        "    \"\"\"\n",
        "    tokens is a 1d numpy array of stroke tokens to be converted into strokes\n",
        "    Returns a list of the strokes\n",
        "    \"\"\"\n",
        "    strokes = []\n",
        "    for token in tokens:\n",
        "        stroke = [0] * 5\n",
        "        if token // 1000000 == 1:\n",
        "            stroke[4] = 1\n",
        "            token -= 1000000\n",
        "        elif token // 500000 == 1:\n",
        "            stroke[3] = 1\n",
        "            token -= 500000\n",
        "        else:\n",
        "            stroke[2] = 1\n",
        "            token -= 100\n",
        "        stroke[0] = token // 511 - 255\n",
        "        stroke[1] = token % 511 - 255\n",
        "        strokes.append(stroke)\n",
        "    return np.asarray(strokes)\n",
        "\n",
        "\n",
        "def clean(data, max_length=100):\n",
        "    \"\"\"\n",
        "    Data is a np 3d array of samples in stroke-3 format\n",
        "    Removes all samples with length > max_length\n",
        "    Converts to stroke-5 and pads to max_length\n",
        "    Tokenizes stroke-5 vectors\n",
        "    Scales offsets down by a factor of eight and rounds\n",
        "    Returns tokenized dataset as a np 2d array\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "    for sample in data:\n",
        "        if len(sample) <= max_length:\n",
        "            sample = to_big_strokes(sample, max_length)\n",
        "            dataset.append(sample)\n",
        "    dataset = np.asarray(dataset)\n",
        "    #dataset[:, :, :2] /= 8\n",
        "    dataset = np.round(dataset)\n",
        "    return dataset\n",
        "\n",
        "class Dataset:\n",
        "    \"\"\" Document later \"\"\"\n",
        "\n",
        "    def __init__(self, filepath, batch_size=32, max_length=250):\n",
        "        \"\"\" Init \"\"\"\n",
        "        data = np.load(\n",
        "            filepath,\n",
        "            encoding='latin1',\n",
        "            allow_pickle=True\n",
        "        )\n",
        "\n",
        "        # Clean up dataset, removing samples over max_length\n",
        "        # and tokenizing\n",
        "        self.train = clean(data['train'])\n",
        "        self.valid = clean(data['valid'])\n",
        "        self.test = clean(data['test'])\n",
        "\n",
        "        # Convert to tensorflow datasets for training\n",
        "        self.train = tf.convert_to_tensor(self.train)\n",
        "        self.train = tf.data.Dataset.from_tensor_slices(list(self.train))\n",
        "        self.valid = tf.convert_to_tensor(self.valid)\n",
        "        self.valid = tf.data.Dataset.from_tensor_slices(list(self.valid))\n",
        "        self.test = tf.convert_to_tensor(self.test)\n",
        "        self.test = tf.data.Dataset.from_tensor_slices(list(self.test))\n",
        "\n",
        "        # Shuffle and batch train and valid sets\n",
        "        self.train = self.train.shuffle(max_length)\n",
        "        self.valid = self.valid.shuffle(max_length)\n",
        "        self.train = self.train.batch(batch_size)\n",
        "        self.valid = self.valid.batch(batch_size)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG1ntY420BkI"
      },
      "source": [
        "def create_masks(inputs, target):\n",
        "    \"\"\"\n",
        "    Creates all masks for training/validation\n",
        "    inputs: tf.Tensor of shape (batch_size, seq_len_in) that contains the input\n",
        "        sentence\n",
        "    target: tf.Tensor of shape (batch_size, seq_len_out) that contains the\n",
        "        target sentence\n",
        "    Returns: encoder_mask, combined_mask, decoder_mask\n",
        "        encoder_mask: tf.Tensor padding mask of shape\n",
        "            (batch_size, 1, 1, seq_len_in) to be applied to the encoder\n",
        "        combined_mask: tf.Tensor of shape\n",
        "            (batch_size, 1, seq_len_out, seq_len_out) used in the first\n",
        "            attention block in the decoder to pad and mask future tokens in the\n",
        "            input received by the decoder. It takes the maximum between a\n",
        "            look ahead mask and the decoder target padding mask.\n",
        "        decoder_mask: tf.Tensor padding mask of shape\n",
        "            (batch_size, 1, 1, seq_len_in) used in the second attention block\n",
        "            in the decoder.\n",
        "    \"\"\"\n",
        "    inputs = tf.reduce_sum(inputs, axis=-1)\n",
        "    target = tf.reduce_sum(target, axis=-1)\n",
        "\n",
        "    encoder_mask = tf.cast(tf.math.equal(inputs, 1), tf.float32)\n",
        "    encoder_mask = encoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "    decoder_mask = tf.identity(encoder_mask)\n",
        "\n",
        "\n",
        "    decoder_target_mask = tf.cast(tf.math.equal(target, 1), tf.float32)\n",
        "    decoder_target_mask = decoder_target_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    target_seq_len = tf.shape(target)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(\n",
        "        tf.ones((target_seq_len, target_seq_len)), -1, 0\n",
        "    )\n",
        "\n",
        "    combined_mask = tf.maximum(decoder_target_mask, look_ahead_mask)\n",
        "\n",
        "    return encoder_mask, combined_mask, decoder_mask"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CdDdtdm52g7"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzlDRMex58lD"
      },
      "source": [
        "def train_transformer(N, dm, h, hidden, max_len, batch_size, epochs):\n",
        "    \"\"\" creates and trains a transformer model for machine translation\n",
        "          of portuguese to English\n",
        "        N: number of encoder and decoder blocks\n",
        "        dm: dimensionality of the model\n",
        "        h: number of heads\n",
        "        hidden: number of hidden units in fc layers\n",
        "        max_len: max tokens per sequence\n",
        "        batch_size: batch size for training\n",
        "        epochs: number of epochs\n",
        "        adam opt (beta1=0.9, beta2=0.98, epsilon=1e-9)\n",
        "        also special learning rat eequation\n",
        "        sparse categorical crossentropy\n",
        "        Returns: the trained model\n",
        "    \"\"\"\n",
        "    # create dataset\n",
        "    data = Dataset('cat.npz', batch_size=batch_size)\n",
        "    \n",
        "    # Create model\n",
        "    transformer = Transformer(\n",
        "        N,\n",
        "        dm,\n",
        "        h,\n",
        "        hidden,\n",
        "        max_len,\n",
        "        max_len\n",
        "    )\n",
        "\n",
        "    # create list to track loss history\n",
        "    loss_history = []\n",
        "    \n",
        "    # create loss objects\n",
        "    mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "    ce_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    def create_mask(batch_size, seq_len):\n",
        "        \"\"\"\n",
        "        Creates the look mask for attention in the decoder\n",
        "        seq_len: Length of the sequence for which to make the mask\n",
        "        \"\"\"\n",
        "        \n",
        "        mask = 1 - tf.linalg.band_part(tf.ones((1, 1, seq_len, seq_len)), -1, 0)\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def loss_function(real, pred):\n",
        "        \"\"\" custom loss function for transformer \n",
        "            real are real values of output\n",
        "            pred: output of the model\n",
        "        \"\"\"\n",
        "        #labels = create_labels(real)\n",
        "        #for n in range(len(labels)):\n",
        "        #    labels[n] = tf.cast(labels[n], dtype=pred[0].dtype)\n",
        "\n",
        "        #x_loss = tf.reduce_sum(loss_(labels[0], pred[0])) / tf.reduce_sum(labels[0])\n",
        "        #y_loss = tf.reduce_sum(loss_(labels[1], pred[1])) / tf.reduce_sum(labels[1])\n",
        "        #pen_loss = tf.reduce_sum(loss_(labels[2], pred[2])) / tf.reduce_sum(labels[2])\n",
        "\n",
        "        #loss = x_loss + y_loss + pen_loss\n",
        "\n",
        "        #mse_loss = loss_(real[:, :, :2], pred[:, :, :2])\n",
        "        #cce_loss = ce_loss(real[:, :, 2:], pred[:, :, 2:])\n",
        "\n",
        "        return mse_loss(real, pred)\n",
        "\n",
        "      # can cast if necessary like so\n",
        "      # mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "      # loss_ *= mask\n",
        "      # do we still need to / by reduced sum\n",
        "      \n",
        "\n",
        "    # set some hyper param stuff\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    learning_rate = CustomSchedule(dm)\n",
        "    # alternative to custom \n",
        "    # learning_rate = 0.005\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    \n",
        "    # train step f'n\n",
        "    #@tf.function(input_signature=train_step_signature)\n",
        "    def train_step(inp, tar):\n",
        "      \"\"\" single train step \n",
        "          takes in input and a target value\n",
        "      \"\"\"\n",
        "      # get inp/tar in right form\n",
        "      # i think this is wrong unfortunately\n",
        "      tar_inp = tar[:, :-1]\n",
        "      tar_real = tar[:, 1:]\n",
        "      \n",
        "      #enc_inp = inp\n",
        "      enc_inp = inp[:, :20]\n",
        "\n",
        "      # create masks\n",
        "      encoder_mask, combined_mask, decoder_mask = create_masks(enc_inp, tar_inp)\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = transformer(\n",
        "                enc_inp,\n",
        "                tar_inp,\n",
        "                True,\n",
        "                encoder_mask,\n",
        "                combined_mask,\n",
        "                decoder_mask\n",
        "            )\n",
        "\n",
        "          # must also take third arg for this loss f'n\n",
        "          # loss = loss_function(tar_real, predictions)\n",
        "          loss = loss_function(tar_real, predictions)\n",
        "\n",
        "      grads = tape.gradient(loss, transformer.trainable_variables)\n",
        "      optimizer.apply_gradients(\n",
        "          zip(grads, transformer.trainable_variables))\n",
        "\n",
        "      train_loss(loss)\n",
        "      loss_history.append(train_loss.result())\n",
        "\n",
        "    # then do the training\n",
        "    for epoch in range(epochs):\n",
        "        train_loss.reset_states()\n",
        "        for batch, inp in enumerate(data.train):\n",
        "            train_step(inp, inp)\n",
        "\n",
        "            if batch % 50 == 0:\n",
        "                if batch % 50 == 0:\n",
        "                    print(\"Epoch {}, batch {}: Loss: {}\".format(\n",
        "                        epoch + 1,\n",
        "                        batch,\n",
        "                        train_loss.result()\n",
        "                    ))\n",
        "        print(\"Epoch {}: loss: {}\".format(\n",
        "            epoch + 1,\n",
        "            train_loss.result()\n",
        "        ))\n",
        "    return transformer, loss_history\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\" Custom learning rate schedule \"\"\"\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        \"\"\" Init \"\"\"\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        \"\"\" Call \"\"\"\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "def retrain(transformer, batch_size, epochs):\n",
        "    \"\"\" continue training a previously trained model \"\"\"\n",
        "    # create dataset\n",
        "    data = Dataset('cat.npz', batch_size=batch_size)\n",
        "\n",
        "    # create new list to track loss history\n",
        "    loss_history = []\n",
        "    \n",
        "    # create loss objects\n",
        "    mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "    ce_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    def loss_function(real, pred):\n",
        "        \"\"\" custom loss function for transformer \n",
        "            real are real values of output\n",
        "            pred: output of the model\n",
        "        \"\"\"\n",
        "        return mse_loss(real, pred)\n",
        "\n",
        "    # set some hyper param stuff\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    learning_rate = CustomSchedule(transformer.encoder.dm)\n",
        "    # alternative to custom \n",
        "    # learning_rate = 0.005\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "    def train_step(inp, tar):\n",
        "      \"\"\" single train step \n",
        "          takes in input and a target value\n",
        "      \"\"\"\n",
        "      # get inp/tar in right form\n",
        "      # i think this is wrong unfortunately\n",
        "      tar_inp = tar[:, :-1]\n",
        "      tar_real = tar[:, 1:]\n",
        "      \n",
        "      #enc_inp = inp\n",
        "      enc_inp = inp[:, :20]\n",
        "\n",
        "      # create masks\n",
        "      encoder_mask, combined_mask, decoder_mask = create_masks(enc_inp, tar_inp)\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = transformer(\n",
        "                enc_inp,\n",
        "                tar_inp,\n",
        "                True,\n",
        "                encoder_mask,\n",
        "                combined_mask,\n",
        "                decoder_mask\n",
        "            )\n",
        "\n",
        "          # must also take third arg for this loss f'n\n",
        "          # loss = loss_function(tar_real, predictions)\n",
        "          loss = loss_function(tar_real, predictions)\n",
        "\n",
        "      grads = tape.gradient(loss, transformer.trainable_variables)\n",
        "      optimizer.apply_gradients(\n",
        "          zip(grads, transformer.trainable_variables))\n",
        "\n",
        "      train_loss(loss)\n",
        "      loss_history.append(train_loss.result())\n",
        "\n",
        "    # then do the training\n",
        "    for epoch in range(epochs):\n",
        "        train_loss.reset_states()\n",
        "        for batch, inp in enumerate(data.train):\n",
        "            train_step(inp, inp)\n",
        "\n",
        "            if batch % 50 == 0:\n",
        "                if batch % 50 == 0:\n",
        "                    print(\"Epoch {}, batch {}: Loss: {}\".format(\n",
        "                        epoch + 1,\n",
        "                        batch,\n",
        "                        train_loss.result()\n",
        "                    ))\n",
        "        print(\"Epoch {}: loss: {}\".format(\n",
        "            epoch + 1,\n",
        "            train_loss.result()\n",
        "        ))\n",
        "    return transformer, loss_history"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Th3aUlCxD5"
      },
      "source": [
        "# train testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho30tLjpA2_E",
        "outputId": "8cb8e7cb-488e-4cb1-f580-336ffd195529"
      },
      "source": [
        "model, loss_history = train_transformer(6, 256, 32, 512, 100, 64, 5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, batch 0: Loss: 482.5292053222656\n",
            "Epoch 1, batch 50: Loss: 457.73980712890625\n",
            "Epoch 1, batch 100: Loss: 457.13836669921875\n",
            "Epoch 1, batch 150: Loss: 448.1417236328125\n",
            "Epoch 1, batch 200: Loss: 439.9256896972656\n",
            "Epoch 1, batch 250: Loss: 432.4217224121094\n",
            "Epoch 1, batch 300: Loss: 427.70806884765625\n",
            "Epoch 1, batch 350: Loss: 422.7500915527344\n",
            "Epoch 1, batch 400: Loss: 417.0492858886719\n",
            "Epoch 1, batch 450: Loss: 413.74169921875\n",
            "Epoch 1, batch 500: Loss: 409.9502868652344\n",
            "Epoch 1, batch 550: Loss: 406.8734436035156\n",
            "Epoch 1, batch 600: Loss: 404.14251708984375\n",
            "Epoch 1, batch 650: Loss: 401.2210693359375\n",
            "Epoch 1, batch 700: Loss: 398.7089538574219\n",
            "Epoch 1, batch 750: Loss: 396.69439697265625\n",
            "Epoch 1, batch 800: Loss: 395.02105712890625\n",
            "Epoch 1, batch 850: Loss: 393.0928649902344\n",
            "Epoch 1, batch 900: Loss: 391.1784362792969\n",
            "Epoch 1, batch 950: Loss: 390.0386047363281\n",
            "Epoch 1: loss: 389.060546875\n",
            "Epoch 2, batch 0: Loss: 350.3294372558594\n",
            "Epoch 2, batch 50: Loss: 365.2881164550781\n",
            "Epoch 2, batch 100: Loss: 366.67041015625\n",
            "Epoch 2, batch 150: Loss: 361.8304748535156\n",
            "Epoch 2, batch 200: Loss: 360.30401611328125\n",
            "Epoch 2, batch 250: Loss: 358.824951171875\n",
            "Epoch 2, batch 300: Loss: 359.0763244628906\n",
            "Epoch 2, batch 350: Loss: 358.2305603027344\n",
            "Epoch 2, batch 400: Loss: 355.6304931640625\n",
            "Epoch 2, batch 450: Loss: 355.0020446777344\n",
            "Epoch 2, batch 500: Loss: 353.4208984375\n",
            "Epoch 2, batch 550: Loss: 352.401123046875\n",
            "Epoch 2, batch 600: Loss: 351.5970764160156\n",
            "Epoch 2, batch 650: Loss: 350.5234375\n",
            "Epoch 2, batch 700: Loss: 349.3309326171875\n",
            "Epoch 2, batch 750: Loss: 348.4192199707031\n",
            "Epoch 2, batch 800: Loss: 348.30133056640625\n",
            "Epoch 2, batch 850: Loss: 347.63043212890625\n",
            "Epoch 2, batch 900: Loss: 346.8987731933594\n",
            "Epoch 2, batch 950: Loss: 346.62353515625\n",
            "Epoch 2: loss: 346.1278991699219\n",
            "Epoch 3, batch 0: Loss: 388.86553955078125\n",
            "Epoch 3, batch 50: Loss: 341.8455810546875\n",
            "Epoch 3, batch 100: Loss: 342.1087951660156\n",
            "Epoch 3, batch 150: Loss: 338.3442687988281\n",
            "Epoch 3, batch 200: Loss: 336.0484924316406\n",
            "Epoch 3, batch 250: Loss: 334.26312255859375\n",
            "Epoch 3, batch 300: Loss: 334.1768798828125\n",
            "Epoch 3, batch 350: Loss: 332.2308044433594\n",
            "Epoch 3, batch 400: Loss: 329.4145812988281\n",
            "Epoch 3, batch 450: Loss: 328.1885986328125\n",
            "Epoch 3, batch 500: Loss: 326.3599853515625\n",
            "Epoch 3, batch 550: Loss: 324.4550476074219\n",
            "Epoch 3, batch 600: Loss: 322.6835632324219\n",
            "Epoch 3, batch 650: Loss: 320.7851257324219\n",
            "Epoch 3, batch 700: Loss: 318.9429931640625\n",
            "Epoch 3, batch 750: Loss: 316.83648681640625\n",
            "Epoch 3, batch 800: Loss: 316.28094482421875\n",
            "Epoch 3, batch 850: Loss: 314.5074768066406\n",
            "Epoch 3, batch 900: Loss: 312.2420654296875\n",
            "Epoch 3, batch 950: Loss: 310.93109130859375\n",
            "Epoch 3: loss: 309.9431457519531\n",
            "Epoch 4, batch 0: Loss: 318.3721618652344\n",
            "Epoch 4, batch 50: Loss: 283.8699951171875\n",
            "Epoch 4, batch 100: Loss: 283.7829284667969\n",
            "Epoch 4, batch 150: Loss: 280.8575134277344\n",
            "Epoch 4, batch 200: Loss: 278.724609375\n",
            "Epoch 4, batch 250: Loss: 275.8754577636719\n",
            "Epoch 4, batch 300: Loss: 275.83453369140625\n",
            "Epoch 4, batch 350: Loss: 274.19122314453125\n",
            "Epoch 4, batch 400: Loss: 271.7486877441406\n",
            "Epoch 4, batch 450: Loss: 270.5273742675781\n",
            "Epoch 4, batch 500: Loss: 268.9286193847656\n",
            "Epoch 4, batch 550: Loss: 267.3503723144531\n",
            "Epoch 4, batch 600: Loss: 267.7901306152344\n",
            "Epoch 4, batch 650: Loss: 267.39007568359375\n",
            "Epoch 4, batch 700: Loss: 265.6132507324219\n",
            "Epoch 4, batch 750: Loss: 264.33685302734375\n",
            "Epoch 4, batch 800: Loss: 263.6950378417969\n",
            "Epoch 4, batch 850: Loss: 262.5593566894531\n",
            "Epoch 4, batch 900: Loss: 261.0977783203125\n",
            "Epoch 4, batch 950: Loss: 260.74859619140625\n",
            "Epoch 4: loss: 260.2525634765625\n",
            "Epoch 5, batch 0: Loss: 208.22698974609375\n",
            "Epoch 5, batch 50: Loss: 259.552978515625\n",
            "Epoch 5, batch 100: Loss: 258.709716796875\n",
            "Epoch 5, batch 150: Loss: 251.8377227783203\n",
            "Epoch 5, batch 200: Loss: 249.30709838867188\n",
            "Epoch 5, batch 250: Loss: 247.5061492919922\n",
            "Epoch 5, batch 300: Loss: 247.1914520263672\n",
            "Epoch 5, batch 350: Loss: 245.41761779785156\n",
            "Epoch 5, batch 400: Loss: 243.18272399902344\n",
            "Epoch 5, batch 450: Loss: 241.8916778564453\n",
            "Epoch 5, batch 500: Loss: 240.57443237304688\n",
            "Epoch 5, batch 550: Loss: 239.05299377441406\n",
            "Epoch 5, batch 600: Loss: 237.96151733398438\n",
            "Epoch 5, batch 650: Loss: 237.1476287841797\n",
            "Epoch 5, batch 700: Loss: 236.51589965820312\n",
            "Epoch 5, batch 750: Loss: 235.45416259765625\n",
            "Epoch 5, batch 800: Loss: 235.0042724609375\n",
            "Epoch 5, batch 850: Loss: 234.44053649902344\n",
            "Epoch 5, batch 900: Loss: 233.60076904296875\n",
            "Epoch 5, batch 950: Loss: 233.2104949951172\n",
            "Epoch 5: loss: 232.91897583007812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-maOSXE3cUxo",
        "outputId": "90599cf6-39cf-4f91-c8a2-3a4379aa7766"
      },
      "source": [
        "model, loss_history1 = retrain(model, 64, 3)\n",
        "plt.plot(range(len(loss_history1)), loss_history)\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, batch 0: Loss: 175.93722534179688\n",
            "Epoch 1, batch 50: Loss: 197.6011962890625\n",
            "Epoch 1, batch 100: Loss: 197.95230102539062\n",
            "Epoch 1, batch 150: Loss: 197.2672576904297\n",
            "Epoch 1, batch 200: Loss: 195.95025634765625\n",
            "Epoch 1, batch 250: Loss: 194.91566467285156\n",
            "Epoch 1, batch 300: Loss: 195.669921875\n",
            "Epoch 1, batch 350: Loss: 195.4561004638672\n",
            "Epoch 1, batch 400: Loss: 194.24118041992188\n",
            "Epoch 1, batch 450: Loss: 194.35206604003906\n",
            "Epoch 1, batch 500: Loss: 194.4602813720703\n",
            "Epoch 1, batch 550: Loss: 193.95310974121094\n",
            "Epoch 1, batch 600: Loss: 193.9517364501953\n",
            "Epoch 1, batch 650: Loss: 193.41334533691406\n",
            "Epoch 1, batch 700: Loss: 193.0749969482422\n",
            "Epoch 1, batch 750: Loss: 192.80638122558594\n",
            "Epoch 1, batch 800: Loss: 193.0418243408203\n",
            "Epoch 1, batch 850: Loss: 193.0738983154297\n",
            "Epoch 1, batch 900: Loss: 192.78176879882812\n",
            "Epoch 1, batch 950: Loss: 192.85968017578125\n",
            "Epoch 1: loss: 192.77914428710938\n",
            "Epoch 2, batch 0: Loss: 204.80752563476562\n",
            "Epoch 2, batch 50: Loss: 194.46026611328125\n",
            "Epoch 2, batch 100: Loss: 198.11865234375\n",
            "Epoch 2, batch 150: Loss: 198.43907165527344\n",
            "Epoch 2, batch 200: Loss: 198.4684600830078\n",
            "Epoch 2, batch 250: Loss: 197.1815643310547\n",
            "Epoch 2, batch 300: Loss: 198.74147033691406\n",
            "Epoch 2, batch 350: Loss: 198.9024658203125\n",
            "Epoch 2, batch 400: Loss: 198.1951904296875\n",
            "Epoch 2, batch 450: Loss: 198.159912109375\n",
            "Epoch 2, batch 500: Loss: 198.56521606445312\n",
            "Epoch 2, batch 550: Loss: 199.09132385253906\n",
            "Epoch 2, batch 600: Loss: 199.1395263671875\n",
            "Epoch 2, batch 650: Loss: 198.75514221191406\n",
            "Epoch 2, batch 700: Loss: 198.735595703125\n",
            "Epoch 2, batch 750: Loss: 198.41592407226562\n",
            "Epoch 2, batch 800: Loss: 198.5750274658203\n",
            "Epoch 2, batch 850: Loss: 198.6602783203125\n",
            "Epoch 2, batch 900: Loss: 198.25038146972656\n",
            "Epoch 2, batch 950: Loss: 198.42745971679688\n",
            "Epoch 2: loss: 198.5259246826172\n",
            "Epoch 3, batch 0: Loss: 169.61012268066406\n",
            "Epoch 3, batch 50: Loss: 200.80906677246094\n",
            "Epoch 3, batch 100: Loss: 205.00389099121094\n",
            "Epoch 3, batch 150: Loss: 205.31602478027344\n",
            "Epoch 3, batch 200: Loss: 205.0273895263672\n",
            "Epoch 3, batch 250: Loss: 203.6458282470703\n",
            "Epoch 3, batch 300: Loss: 205.40716552734375\n",
            "Epoch 3, batch 350: Loss: 205.37103271484375\n",
            "Epoch 3, batch 400: Loss: 204.26161193847656\n",
            "Epoch 3, batch 450: Loss: 204.27447509765625\n",
            "Epoch 3, batch 500: Loss: 204.9207305908203\n",
            "Epoch 3, batch 550: Loss: 204.691650390625\n",
            "Epoch 3, batch 600: Loss: 204.79498291015625\n",
            "Epoch 3, batch 650: Loss: 204.47335815429688\n",
            "Epoch 3, batch 700: Loss: 204.19940185546875\n",
            "Epoch 3, batch 750: Loss: 204.11886596679688\n",
            "Epoch 3, batch 800: Loss: 204.46153259277344\n",
            "Epoch 3, batch 850: Loss: 204.32723999023438\n",
            "Epoch 3, batch 900: Loss: 203.7832794189453\n",
            "Epoch 3, batch 950: Loss: 204.10801696777344\n",
            "Epoch 3: loss: 204.2776641845703\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "R0xDvShHnsrY",
        "outputId": "73f41f67-6b1e-4200-c6d4-8cb6a2f0fa02"
      },
      "source": [
        "plt.plot(range(len(loss_history1)), loss_history1)\n",
        "plt.show()\n",
        "model.save_weights(\"multihead_weights\")\n",
        "# model.save(\"multihead_model\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "tKbRjfoRiIyp",
        "outputId": "9903c852-9021-498a-8306-7255927f1f9f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(loss_history)), loss_history)\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "spOV8PfTIzBD",
        "outputId": "d53252a3-86ae-4007-eb5b-1c09c5c16f87"
      },
      "source": [
        "### this is our test file for fisplay after training\n",
        "# test = data['train'][4]\n",
        "\n",
        "def to_big_strokes(stroke, max_len=100):\n",
        "    \"\"\"Converts from stroke-3 to stroke-5 format and pads to given length.\"\"\"\n",
        "    # (But does not insert special start token).\n",
        "\n",
        "    result = np.zeros((max_len, 5), dtype=float)\n",
        "    l = len(stroke)\n",
        "    assert l <= max_len\n",
        "    result[0:l, 0:2] = stroke[:, 0:2]\n",
        "    result[0:l, 3] = stroke[:, 2]\n",
        "    result[0:l, 2] = 1 - result[0:l, 3]\n",
        "    result[l:, 4] = 1\n",
        "    return result\n",
        "\n",
        "breakoff = 35\n",
        "\n",
        "data = np.load('cat.npz', encoding='latin1', allow_pickle=True)\n",
        "test = data['test'][4]\n",
        "test = to_big_strokes(test)[np.newaxis, :, :]\n",
        "print(test.shape)\n",
        "inputs = test[:, :breakoff]\n",
        "prediction = test[:, :breakoff]\n",
        "\n",
        "while prediction[0, -1, -1] != 1 and len(prediction[0]) < 100:\n",
        "    encoder_mask, combined_mask, decoder_mask = create_masks(inputs, prediction)\n",
        "    output = model(\n",
        "        inputs,\n",
        "        prediction,\n",
        "        True,\n",
        "        encoder_mask,\n",
        "        combined_mask,\n",
        "        decoder_mask\n",
        "    )[0].numpy()\n",
        "\n",
        "    #output = np.round(output)\n",
        "\n",
        "    offsets = np.round(output[-1, :2])\n",
        "\n",
        "    pen_state = output[-1, 2:]\n",
        "    #maxes = np.amax(pen_states, axis=1)\n",
        "    pen_state = np.where(pen_state == np.amax(pen_state), 1, 0)\n",
        "\n",
        "    output = np.concatenate((offsets, pen_state)).reshape((1, 1, 5))\n",
        "\n",
        "    prediction = np.concatenate((prediction, output), axis=1)\n",
        "\n",
        "\n",
        "for i in range(len(prediction[0])):\n",
        "    if i == breakoff:\n",
        "        print(\"Predictions begin here\")\n",
        "    print(\"True: {}\".format(test[0, i]))\n",
        "    print(\"Pred: {}\".format(prediction[0, i]))\n",
        "    print(\"\")\n",
        "\n",
        "#print(maxes)\n",
        "\n",
        "#maxes = np.amax(predictions, axis=1).reshape((99, 1))\n",
        "#predictions = np.where(predictions == maxes, 1, 0)\n",
        "#print(predictions)\n",
        "#print(maxes)\n",
        "\n",
        "#for row in inputs:\n",
        "#    print(row)\n",
        "\n",
        "draw_strokes(test[0], svg_filename='realcat.svg')\n",
        "draw_strokes(test[0, :breakoff], svg_filename='given_points.svg')\n",
        "draw_strokes(prediction[0], svg_filename=\"cat.svg\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 100, 5)\n",
            "True: [-18.   5.   1.   0.   0.]\n",
            "Pred: [-18.   5.   1.   0.   0.]\n",
            "\n",
            "True: [-15.  16.   1.   0.   0.]\n",
            "Pred: [-15.  16.   1.   0.   0.]\n",
            "\n",
            "True: [-7. 13.  1.  0.  0.]\n",
            "Pred: [-7. 13.  1.  0.  0.]\n",
            "\n",
            "True: [-1. 36.  1.  0.  0.]\n",
            "Pred: [-1. 36.  1.  0.  0.]\n",
            "\n",
            "True: [ 9. 15.  1.  0.  0.]\n",
            "Pred: [ 9. 15.  1.  0.  0.]\n",
            "\n",
            "True: [28. 26.  1.  0.  0.]\n",
            "Pred: [28. 26.  1.  0.  0.]\n",
            "\n",
            "True: [13.  4.  1.  0.  0.]\n",
            "Pred: [13.  4.  1.  0.  0.]\n",
            "\n",
            "True: [29. -1.  1.  0.  0.]\n",
            "Pred: [29. -1.  1.  0.  0.]\n",
            "\n",
            "True: [ 32. -15.   1.   0.   0.]\n",
            "Pred: [ 32. -15.   1.   0.   0.]\n",
            "\n",
            "True: [ 24. -22.   1.   0.   0.]\n",
            "Pred: [ 24. -22.   1.   0.   0.]\n",
            "\n",
            "True: [ 11. -16.   1.   0.   0.]\n",
            "Pred: [ 11. -16.   1.   0.   0.]\n",
            "\n",
            "True: [  3. -10.   1.   0.   0.]\n",
            "Pred: [  3. -10.   1.   0.   0.]\n",
            "\n",
            "True: [ -2. -23.   1.   0.   0.]\n",
            "Pred: [ -2. -23.   1.   0.   0.]\n",
            "\n",
            "True: [-12. -18.   1.   0.   0.]\n",
            "Pred: [-12. -18.   1.   0.   0.]\n",
            "\n",
            "True: [-24. -12.   1.   0.   0.]\n",
            "Pred: [-24. -12.   1.   0.   0.]\n",
            "\n",
            "True: [-21.  -3.   1.   0.   0.]\n",
            "Pred: [-21.  -3.   1.   0.   0.]\n",
            "\n",
            "True: [-48.   0.   1.   0.   0.]\n",
            "Pred: [-48.   0.   1.   0.   0.]\n",
            "\n",
            "True: [-19.   9.   0.   1.   0.]\n",
            "Pred: [-19.   9.   0.   1.   0.]\n",
            "\n",
            "True: [-14.   1.   1.   0.   0.]\n",
            "Pred: [-14.   1.   1.   0.   0.]\n",
            "\n",
            "True: [  9. -35.   1.   0.   0.]\n",
            "Pred: [  9. -35.   1.   0.   0.]\n",
            "\n",
            "True: [ 5. -9.  1.  0.  0.]\n",
            "Pred: [ 5. -9.  1.  0.  0.]\n",
            "\n",
            "True: [  3. -31.   1.   0.   0.]\n",
            "Pred: [  3. -31.   1.   0.   0.]\n",
            "\n",
            "True: [  8. -24.   1.   0.   0.]\n",
            "Pred: [  8. -24.   1.   0.   0.]\n",
            "\n",
            "True: [5. 7. 1. 0. 0.]\n",
            "Pred: [5. 7. 1. 0. 0.]\n",
            "\n",
            "True: [ 6. 39.  1.  0.  0.]\n",
            "Pred: [ 6. 39.  1.  0.  0.]\n",
            "\n",
            "True: [19. 39.  0.  1.  0.]\n",
            "Pred: [19. 39.  0.  1.  0.]\n",
            "\n",
            "True: [35.  5.  1.  0.  0.]\n",
            "Pred: [35.  5.  1.  0.  0.]\n",
            "\n",
            "True: [ 18. -45.   1.   0.   0.]\n",
            "Pred: [ 18. -45.   1.   0.   0.]\n",
            "\n",
            "True: [  8. -31.   1.   0.   0.]\n",
            "Pred: [  8. -31.   1.   0.   0.]\n",
            "\n",
            "True: [5. 1. 1. 0. 0.]\n",
            "Pred: [5. 1. 1. 0. 0.]\n",
            "\n",
            "True: [ 8. 45.  1.  0.  0.]\n",
            "Pred: [ 8. 45.  1.  0.  0.]\n",
            "\n",
            "True: [ 0. 22.  1.  0.  0.]\n",
            "Pred: [ 0. 22.  1.  0.  0.]\n",
            "\n",
            "True: [-5. 19.  0.  1.  0.]\n",
            "Pred: [-5. 19.  0.  1.  0.]\n",
            "\n",
            "True: [-100.   49.    1.    0.    0.]\n",
            "Pred: [-100.   49.    1.    0.    0.]\n",
            "\n",
            "True: [ 3. 13.  1.  0.  0.]\n",
            "Pred: [ 3. 13.  1.  0.  0.]\n",
            "\n",
            "Predictions begin here\n",
            "True: [5. 6. 1. 0. 0.]\n",
            "Pred: [2. 7. 1. 0. 0.]\n",
            "\n",
            "True: [23. -4.  1.  0.  0.]\n",
            "Pred: [1. 4. 1. 0. 0.]\n",
            "\n",
            "True: [ 6. -9.  1.  0.  0.]\n",
            "Pred: [2. 4. 1. 0. 0.]\n",
            "\n",
            "True: [0. 7. 1. 0. 0.]\n",
            "Pred: [2. 2. 1. 0. 0.]\n",
            "\n",
            "True: [4. 5. 1. 0. 0.]\n",
            "Pred: [ 4. -2.  1.  0.  0.]\n",
            "\n",
            "True: [11.  6.  1.  0.  0.]\n",
            "Pred: [ 0. -3.  1.  0.  0.]\n",
            "\n",
            "True: [5. 0. 1. 0. 0.]\n",
            "Pred: [-2. -3.  1.  0.  0.]\n",
            "\n",
            "True: [ 4. -5.  1.  0.  0.]\n",
            "Pred: [-1. -4.  1.  0.  0.]\n",
            "\n",
            "True: [  0. -13.   0.   1.   0.]\n",
            "Pred: [-3. -3.  1.  0.  0.]\n",
            "\n",
            "True: [-59. -38.   1.   0.   0.]\n",
            "Pred: [-2. -1.  1.  0.  0.]\n",
            "\n",
            "True: [ 6. 20.  0.  1.  0.]\n",
            "Pred: [-2.  1.  1.  0.  0.]\n",
            "\n",
            "True: [ 37. -21.   1.   0.   0.]\n",
            "Pred: [-3.  3.  1.  0.  0.]\n",
            "\n",
            "True: [ 2. 28.  0.  1.  0.]\n",
            "Pred: [-3.  5.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-1.  7.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [5. 7. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [1. 2. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [2. 0. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 1. -1.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 1. -1.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-1. -4.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-2. -3.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-1. -2.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-1. -2.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-1. -4.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-2. -3.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-2. -3.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-4. -1.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-4.  1.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-2.  4.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [1. 8. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [2. 7. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [1. 6. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 4. 10.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [6. 5. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 5. -0.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 5. -1.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 5. -3.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 2. -3.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 0. -4.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-1. -3.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-4. -3.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-3.  1.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-4.  3.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-4.  4.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-1.  5.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [1. 5. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [4. 3. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [4. 0. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 3. -0.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 3. -2.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 2. -5.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 1. -6.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [ 0. -5.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-3. -5.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-5. -2.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-4.  1.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-4.  1.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-2.  3.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-3.  5.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-3.  7.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-2.  8.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-0.  5.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [-1.  5.  1.  0.  0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [3. 5. 1. 0. 0.]\n",
            "\n",
            "True: [0. 0. 0. 0. 1.]\n",
            "Pred: [0. 4. 1. 0. 0.]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"1095.0\" version=\"1.1\" width=\"795.0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"1095.0\" width=\"795.0\" x=\"0\" y=\"0\"/><path d=\"M230.0,495.0 m-90.0,25.0 l-75.0,80.0 -35.0,65.0 l-5.0,180.0 45.0,75.0 l140.0,130.0 65.0,20.0 l145.0,-5.0 160.0,-75.0 l120.0,-110.0 55.0,-80.0 l15.0,-50.0 -10.0,-115.0 l-60.0,-90.0 -120.0,-60.0 l-105.0,-15.0 -240.0,0.0 l-95.0,45.0 m-70.0,5.0 l45.0,-175.0 25.0,-45.0 l15.0,-155.0 40.0,-120.0 l25.0,35.0 30.0,195.0 l95.0,195.0 m175.0,25.0 l90.0,-225.0 40.0,-155.0 l25.0,5.0 40.0,225.0 l0.0,110.0 -25.0,95.0 m-500.0,245.0 l15.0,65.0 25.0,30.0 l115.0,-20.0 30.0,-45.0 l0.0,35.0 20.0,25.0 l55.0,30.0 25.0,0.0 l20.0,-25.0 0.0,-65.0 m-295.0,-190.0 l30.0,100.0 m185.0,-105.0 l10.0,140.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"1095.0\" version=\"1.1\" width=\"795.0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"1095.0\" width=\"795.0\" x=\"0\" y=\"0\"/><path d=\"M230.0,495.0 m-90.0,25.0 l-75.0,80.0 -35.0,65.0 l-5.0,180.0 45.0,75.0 l140.0,130.0 65.0,20.0 l145.0,-5.0 160.0,-75.0 l120.0,-110.0 55.0,-80.0 l15.0,-50.0 -10.0,-115.0 l-60.0,-90.0 -120.0,-60.0 l-105.0,-15.0 -240.0,0.0 l-95.0,45.0 m-70.0,5.0 l45.0,-175.0 25.0,-45.0 l15.0,-155.0 40.0,-120.0 l25.0,35.0 30.0,195.0 l95.0,195.0 m175.0,25.0 l90.0,-225.0 40.0,-155.0 l25.0,5.0 40.0,225.0 l0.0,110.0 -25.0,95.0 m-500.0,245.0 l15.0,65.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"1195.0\" version=\"1.1\" width=\"795.0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"1195.0\" width=\"795.0\" x=\"0\" y=\"0\"/><path d=\"M230.0,495.0 m-90.0,25.0 l-75.0,80.0 -35.0,65.0 l-5.0,180.0 45.0,75.0 l140.0,130.0 65.0,20.0 l145.0,-5.0 160.0,-75.0 l120.0,-110.0 55.0,-80.0 l15.0,-50.0 -10.0,-115.0 l-60.0,-90.0 -120.0,-60.0 l-105.0,-15.0 -240.0,0.0 l-95.0,45.0 m-70.0,5.0 l45.0,-175.0 25.0,-45.0 l15.0,-155.0 40.0,-120.0 l25.0,35.0 30.0,195.0 l95.0,195.0 m175.0,25.0 l90.0,-225.0 40.0,-155.0 l25.0,5.0 40.0,225.0 l0.0,110.0 -25.0,95.0 m-500.0,245.0 l15.0,65.0 10.0,35.0 l5.0,20.0 10.0,20.0 l10.0,10.0 20.0,-10.0 l0.0,-15.0 -10.0,-15.0 l-5.0,-20.0 -15.0,-15.0 l-10.0,-5.0 -10.0,5.0 l-15.0,15.0 -15.0,25.0 l-5.0,35.0 25.0,35.0 l5.0,10.0 10.0,0.0 l5.0,-5.0 5.0,-5.0 l-5.0,-20.0 -10.0,-15.0 l-5.0,-10.0 -5.0,-10.0 l-5.0,-20.0 -10.0,-15.0 l-10.0,-15.0 -20.0,-5.0 l-20.0,5.0 -10.0,20.0 l5.0,40.0 10.0,35.0 l5.0,30.0 20.0,50.0 l30.0,25.0 25.0,-0.0 l25.0,-5.0 25.0,-15.0 l10.0,-15.0 0.0,-20.0 l-5.0,-15.0 -20.0,-15.0 l-15.0,5.0 -20.0,15.0 l-20.0,20.0 -5.0,25.0 l5.0,25.0 20.0,15.0 l20.0,0.0 15.0,-0.0 l15.0,-10.0 10.0,-25.0 l5.0,-30.0 0.0,-25.0 l-15.0,-25.0 -25.0,-10.0 l-20.0,5.0 -20.0,5.0 l-10.0,15.0 -15.0,25.0 l-15.0,35.0 -10.0,40.0 l-0.0,25.0 -5.0,25.0 l15.0,25.0 0.0,20.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twNEKMkAImxD"
      },
      "source": [
        "# drawing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THhbxBcWIo3v"
      },
      "source": [
        "# libraries required for visualisation:\n",
        "import os\n",
        "import svgwrite\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from IPython.display import SVG, display\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "# import data_Manager\n",
        "import math\n",
        "from matplotlib import animation\n",
        "\n",
        "# data_Manager = Data\n",
        "# set numpy output to something sensible\n",
        "np.set_printoptions(precision=8, edgeitems=6, linewidth=200, suppress=True)\n",
        "\n",
        "\n",
        "def get_bounds(data, factor=10):\n",
        "    \"\"\"Return bounds of data.\"\"\"\n",
        "    min_x = 0\n",
        "    max_x = 0\n",
        "    min_y = 0\n",
        "    max_y = 0\n",
        "\n",
        "    abs_x = 0\n",
        "    abs_y = 0\n",
        "    for i in range(len(data)):\n",
        "        x = float(data[i, 0]) / factor\n",
        "        y = float(data[i, 1]) / factor\n",
        "        abs_x += x\n",
        "        abs_y += y\n",
        "        min_x = min(min_x, abs_x)\n",
        "        min_y = min(min_y, abs_y)\n",
        "        max_x = max(max_x, abs_x)\n",
        "        max_y = max(max_y, abs_y)\n",
        "\n",
        "    return (min_x, max_x, min_y, max_y)\n",
        "\n",
        "\n",
        "def slerp(p0, p1, t):\n",
        "    \"\"\"Spherical interpolation.\"\"\"\n",
        "    omega = np.arccos(np.dot(p0 / np.linalg.norm(p0), p1 / np.linalg.norm(p1)))\n",
        "    so = np.sin(omega)\n",
        "    return np.sin((1.0 - t) * omega) / so * p0 + np.sin(t * omega) / so * p1\n",
        "\n",
        "\n",
        "def lerp(p0, p1, t):\n",
        "    \"\"\"Linear interpolation.\"\"\"\n",
        "    return (1.0 - t) * p0 + t * p1\n",
        "\n",
        "\n",
        "def to_normal_strokes(big_stroke):\n",
        "    \"\"\"Convert from stroke-5 format to stroke-3.\"\"\"\n",
        "    l = 0\n",
        "    for i in range(len(big_stroke)):\n",
        "        if big_stroke[i, 4] > 0:\n",
        "            l = i\n",
        "            break\n",
        "    if l == 0:\n",
        "        l = len(big_stroke)\n",
        "    result = np.zeros((l, 3))\n",
        "    result[:, 0:2] = big_stroke[0:l, 0:2]\n",
        "    result[:, 2] = big_stroke[0:l, 3]\n",
        "    return result\n",
        "\n",
        "\n",
        "# little function that displays vector images and saves them to .svg\n",
        "def draw_strokes(data, factor=0.2, svg_filename = '/tmp/sketch_rnn/svg/sample.svg'):\n",
        "    # data = data_Manager.to_normal_strokes(data)\n",
        "    data = to_normal_strokes(data)\n",
        "    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
        "    dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
        "    dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
        "    dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n",
        "    lift_pen = 1\n",
        "    abs_x = 25 - min_x \n",
        "    abs_y = 25 - min_y\n",
        "    p = \"M%s,%s \" % (abs_x, abs_y)\n",
        "    command = \"m\"\n",
        "    for i in range(len(data)):\n",
        "        if (lift_pen == 1):\n",
        "            command = \"m\"\n",
        "        elif (command != \"l\"):\n",
        "            command = \"l\"\n",
        "        else:\n",
        "            command = \"\"\n",
        "        x = float(data[i,0])/factor\n",
        "        y = float(data[i,1])/factor\n",
        "        lift_pen = data[i, 2]\n",
        "        p += command+str(x)+\",\"+str(y)+\" \"\n",
        "    the_color = \"black\"\n",
        "    stroke_width = 1\n",
        "    dwg.add(dwg.path(p).stroke(the_color,stroke_width).fill(\"none\"))\n",
        "    dwg.save()\n",
        "    display(SVG(dwg.tostring()))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Function for animate drawing. \n",
        "taken from \n",
        "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Strokes_QuickDraw.ipynb#scrollTo=0ABX6O4kYwYS\n",
        "\"\"\"\n",
        "def create_animation(drawing, fps = 30, idx = 0, lw = 5): \n",
        "  \n",
        "  seq_length = 0 \n",
        "  \n",
        "  xmax = 0 \n",
        "  ymax = 0 \n",
        "  \n",
        "  xmin = math.inf\n",
        "  ymin = math.inf\n",
        "  \n",
        "  #retreive min,max and the length of the drawing  \n",
        "  for k in range(0, len(drawing)):\n",
        "    x = drawing[k][0]\n",
        "    y = drawing[k][1]\n",
        "\n",
        "    seq_length += len(x)\n",
        "    xmax = max([max(x), xmax]) \n",
        "    ymax = max([max(y), ymax]) \n",
        "    \n",
        "    xmin = min([min(x), xmin]) \n",
        "    ymin = min([min(y), ymin]) \n",
        "    \n",
        "  i = 0 \n",
        "  j = 0\n",
        "  \n",
        "  # First set up the figure, the axis, and the plot element we want to animate\n",
        "  fig = plt.figure()\n",
        "  ax = plt.axes(xlim=(xmax+lw, xmin-lw), ylim=(ymax+lw, ymin-lw))\n",
        "  ax.set_facecolor(\"white\")\n",
        "  line, = ax.plot([], [], lw=lw)\n",
        "\n",
        "  #remove the axis \n",
        "  ax.grid = False\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  \n",
        "  # initialization function: plot the background of each frame\n",
        "  def init():\n",
        "      line.set_data([], [])\n",
        "      return line, \n",
        "\n",
        "  # animation function.  This is called sequentially\n",
        "  def animate(frame):    \n",
        "    nonlocal i, j, line\n",
        "    x = drawing[i][0]\n",
        "    y = drawing[i][1]\n",
        "    line.set_data(x[0:j], y[0:j])\n",
        "    \n",
        "    if j >= len(x):\n",
        "      i +=1\n",
        "      j = 0 \n",
        "      line, = ax.plot([], [], lw=lw)\n",
        "      \n",
        "    else:\n",
        "      j += 1\n",
        "    return line,\n",
        "  \n",
        "  # call the animator.  blit=True means only re-draw the parts that have changed.\n",
        "  anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                                 frames= seq_length + len(drawing), blit=True)\n",
        "  plt.close()\n",
        "  \n",
        "  # save the animation as an mp4.  \n",
        "  anim.save(f'video.mp4', fps=fps, extra_args=['-vcodec', 'libx264'])"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}