{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Branched Decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnDI7Hog5omz"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Downloads and model classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nidXb-SvzyQN",
        "outputId": "540ad69e-db0b-45c2-dd2a-d40f2b4a8840"
      },
      "source": [
        "!pip install svgwrite\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.1-py3-none-any.whl (66 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 20 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 30 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 61 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 66 kB 2.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite\n",
            "Successfully installed svgwrite-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCatNYia0qcI"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def positional_encoding(max_seq_len, dm):\n",
        "    \"\"\"\n",
        "    Calculates the positional encoding for a transformer\n",
        "    max_seq_len: integer representing the maximum sequence length\n",
        "    dm: integer representing the model depth\n",
        "    Returns: numpy.ndarray of shape (max_seq_len, dm) containing the positional\n",
        "             encoding vectors\n",
        "    \"\"\"\n",
        "    PE = np.zeros((max_seq_len, dm))\n",
        "    for row in range(max_seq_len):\n",
        "        for col in range(0, dm, 2):\n",
        "            PE[row, col] = np.sin(row / (10000 ** (col / dm)))\n",
        "            PE[row, col + 1] = np.cos(row / (10000 ** (col / dm)))\n",
        "    return PE\n",
        "\n",
        "\n",
        "def sdp_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Q: tensor with shape (..., seq_len_q, dk) containing the query matrix\n",
        "    K: tensor with shape (..., seq_len_v, dk) containing the key matrix\n",
        "    V: tensor with shape (..., seq_len_v, dv) containing the value matrix\n",
        "    mask: tensor that can be broadcast into (..., seq_len_q, seq_len_v)\n",
        "          containing the optional maask, or defaulted to None\n",
        "    The Preceding dimensions of Q, K, and V are the same\n",
        "    Returns: output, weights\n",
        "             output: tensor with shape (..., seq_len_q, dv) containing the dot\n",
        "                     product attention\n",
        "             weights: tensor with shape (..., seq_len_q, seq_len_v) containing\n",
        "                      the attention weights\n",
        "    \"\"\"\n",
        "    # Matmul Q and K\n",
        "    QK = tf.matmul(Q, K, transpose_b=True)\n",
        "\n",
        "    # Scale the dot product\n",
        "    dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "    scaled = QK / tf.math.sqrt(dk)\n",
        "\n",
        "    # Add mask if not None\n",
        "    if mask is not None:\n",
        "        scaled += mask * -1e9\n",
        "\n",
        "    # Pass scaled attention through softmax activation\n",
        "    weights = tf.nn.softmax(scaled, axis=-1)\n",
        "\n",
        "    # Matmul by value matrix for output\n",
        "    output = tf.matmul(weights, V)\n",
        "\n",
        "    return output, weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class to perform multi head attention\n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h):\n",
        "        \"\"\"\n",
        "        dm: integer representing the model dimensionality\n",
        "        h: integer representing the number of heads\n",
        "        dm is divisible by h\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.h = h\n",
        "        self.dm = dm\n",
        "        self.depth = dm // self.h\n",
        "        self.Wq = tf.keras.layers.Dense(dm)\n",
        "        self.Wk = tf.keras.layers.Dense(dm)\n",
        "        self.Wv = tf.keras.layers.Dense(dm)\n",
        "        self.linear = tf.keras.layers.Dense(dm)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Splits the last dimension of tensor x into (h, depth)\n",
        "        Transpose the result such that the shape is\n",
        "        (batch_size, h, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.h, self.depth))\n",
        "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        return x\n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        \"\"\"\n",
        "        Q: tensor with shape (..., seq_len_q, dk) containing the query matrix\n",
        "        K: tensor with shape (..., seq_len_v, dk) containing the key matrix\n",
        "        V: tensor with shape (..., seq_len_v, dv) containing the value matrix\n",
        "        mask: always None\n",
        "        The Preceding dimensions of Q, K, and V are the same\n",
        "        Returns: output, weights\n",
        "                 output: tensor with shape (..., seq_len_q, dv) containing the\n",
        "                         dot product attention\n",
        "                 weights: tensor with shape (..., seq_len_q, seq_len_v)\n",
        "                          containing the attention weights\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(Q)[0]\n",
        "\n",
        "        # Generate query, key, and value matrices\n",
        "        Q = self.Wq(Q)\n",
        "        K = self.Wk(K)\n",
        "        V = self.Wv(V)\n",
        "\n",
        "        # Split between heads\n",
        "        Q = self.split_heads(Q, batch_size)\n",
        "        K = self.split_heads(K, batch_size)\n",
        "        V = self.split_heads(V, batch_size)\n",
        "\n",
        "        # Scaled Dot Product Attention\n",
        "        attention, weights = sdp_attention(Q, K, V, mask)\n",
        "\n",
        "        # Refit to pass through linear layer\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        attention = tf.reshape(attention, (batch_size, -1, self.dm))\n",
        "        output = self.linear(attention)\n",
        "\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Class representation of a decoder block for a transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h, hidden, drop_rate=0.1, name=None):\n",
        "        \"\"\"\n",
        "        dm: Dimensionality of the model\n",
        "        h: Number of heads\n",
        "        hidden: Number of hidden units in the fully connected layer\n",
        "        drop_rate: Dropout rate\n",
        "        \"\"\"\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        if name is not None:\n",
        "            self._name = name\n",
        "        self.mha1 = MultiHeadAttention(dm, h)\n",
        "        self.dense_hidden = tf.keras.layers.Dense(\n",
        "            units=hidden,\n",
        "            activation='relu'\n",
        "        )\n",
        "        self.dense_output = tf.keras.layers.Dense(units=dm)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, inputs, look_ahead_mask, training=False):\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch, target_seq_len, dm)containing the input to\n",
        "           the decoder block\n",
        "        training: boolean to determine if the model is training\n",
        "        look_ahead_mask: mask to be applied to the first multi head attention\n",
        "                         layer\n",
        "        Returns: tensor of shape (batch, target_seq_len, dm) containing the\n",
        "                 block's output\n",
        "        \"\"\"\n",
        "        # Pass through MHA and dropout layer\n",
        "        attn_out, _ = self.mha1(inputs, inputs, inputs, look_ahead_mask)\n",
        "        attn_out = self.dropout1(attn_out, training=training)\n",
        "\n",
        "        # Add and normalize\n",
        "        out = self.layernorm1(inputs + attn_out)\n",
        "\n",
        "        # Pass through dense layers and dropout layer\n",
        "        dense_output = self.dense_hidden(out)\n",
        "        dense_output = self.dense_output(dense_output)\n",
        "        dense_output = self.dropout2(dense_output, training=training)\n",
        "\n",
        "        # Add and normalize\n",
        "        out = self.layernorm2(out + dense_output)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Class representation of a decoder for a transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, Nb, No, Np, dm, h, hidden, max_seq_len,\n",
        "                 drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        Nb - number of blocks in the base of the model\n",
        "        No - number of blocks in the x/y offset branch\n",
        "        Np - number of blocks in the pen state prediction branch\n",
        "        dm - the dimensionality of the model\n",
        "        h - the number of heads\n",
        "        hidden - the number of hidden units in the fully connected layer\n",
        "        max_seq_len - the maximum sequence length possible\n",
        "        drop_rate - the dropout rate\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        self.Nb = Nb\n",
        "        self.No = No\n",
        "        self.Np = Np\n",
        "        self.dm = dm\n",
        "        self.projection = tf.keras.layers.Dense(dm, name='base_projection')\n",
        "        self.positional_encoding = positional_encoding(max_seq_len, dm)\n",
        "        self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "        self.base_blocks = [\n",
        "            DecoderBlock(dm, h, hidden, drop_rate,\n",
        "            name=\"base_block_\" + str(n)) for n in range(Nb)\n",
        "        ]\n",
        "        self.offset_blocks = [\n",
        "            DecoderBlock(dm, h, hidden, drop_rate,\n",
        "            name=\"offset_block_\" + str(n)) for n in range(No)\n",
        "        ]\n",
        "        self.pen_blocks = [\n",
        "            DecoderBlock(dm, h, hidden, drop_rate,\n",
        "            name=\"pen_block_\" + str(n)) for n in range(Np)\n",
        "        ]\n",
        "\n",
        "        self.offset_dense = tf.keras.layers.Dense(dm, name='offset_dense')\n",
        "        self.offset_out = tf.keras.layers.Dense(2, name='offset_out')\n",
        "        self.pen_dense = tf.keras.layers.Dense(dm, name='pen_dense')\n",
        "        self.pen_out = tf.keras.layers.Dense(3, name='pen_out',\n",
        "                                             activation='softmax')\n",
        "\n",
        "    def call(self, inputs, look_ahead_mask=None, training=False):\n",
        "        \"\"\"\n",
        "        x - a tensor of shape (batch, target_seq_len, dm) containing the input\n",
        "            to the decoder\n",
        "        encoder_output - a tensor of shape (batch, input_seq_len, dm)\n",
        "            containing the output of the encoder\n",
        "        training - a boolean to determine if the model is training\n",
        "        look_ahead_mask - the mask to be applied to the first multi head\n",
        "            attention layer\n",
        "        padding_mask - the mask to be applied to the second multi head\n",
        "            attention layer\n",
        "        Returns: a tensor of shape (batch, target_seq_len, dm) containing the\n",
        "            decoder output\n",
        "        \"\"\"\n",
        "        seq_len = int(inputs.shape[1])\n",
        "\n",
        "        # Project to model dimension\n",
        "        x = self.projection(inputs)\n",
        "\n",
        "        # Add positional encoding and pass through dropout layer\n",
        "        x *= tf.math.sqrt(tf.cast(self.dm, 'float32'))\n",
        "        x += self.positional_encoding[:seq_len]\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # Pass through base decoder blocks\n",
        "        for block in self.base_blocks:\n",
        "            x = block(x, look_ahead_mask, training)\n",
        "\n",
        "        # Pass through offset branch\n",
        "        offset = x\n",
        "        for block in self.offset_blocks:\n",
        "            offset = block(offset, look_ahead_mask, training)\n",
        "        offset = self.offset_dense(offset)\n",
        "        offset = self.offset_out(offset)\n",
        "\n",
        "        pen = x\n",
        "        for block in self.pen_blocks:\n",
        "            pen = block(pen, look_ahead_mask, training)\n",
        "        pen = self.pen_dense(pen)\n",
        "        pen = self.pen_out(pen)\n",
        "\n",
        "        return offset, pen\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbHaTTmC5ySS"
      },
      "source": [
        "# dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-5QHfuK1Kpp"
      },
      "source": [
        "# Function from utils.py for sketch-rnn in the Magenta github repository\n",
        "# at https://github.com/magenta/magenta/tree/main/magenta/models/sketch_rnn\n",
        "def to_big_strokes(stroke, max_len=250):\n",
        "  \"\"\"Converts from stroke-3 to stroke-5 format and pads to given length.\"\"\"\n",
        "  # (But does not insert special start token).\n",
        "\n",
        "  result = np.zeros((max_len, 5), dtype=float)\n",
        "  l = len(stroke)\n",
        "  assert l <= max_len\n",
        "  result[0:l, 0:2] = stroke[:, 0:2]\n",
        "  result[0:l, 3] = stroke[:, 2]\n",
        "  result[0:l, 2] = 1 - result[0:l, 3]\n",
        "  result[l:, 4] = 1\n",
        "  return result\n",
        "\n",
        "# Tokenizing isn't being used anymore but these functions are being kept\n",
        "# until we're absolutely certain we don't need them\n",
        "\n",
        "def strokes_to_tokens(strokes):\n",
        "    \"\"\"\n",
        "    strokes is a 2d numpy array of stroke-5 vectors to be converted into tokens\n",
        "    Returns a list of the tokenized vectors\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    for stroke in strokes:\n",
        "        token = (stroke[0] + 255) * 511\n",
        "        token += (stroke[1] + 255)\n",
        "        token += np.sum(np.array([100, 500000, 1000000]) * stroke[2:])\n",
        "        tokens.append(token)\n",
        "    return np.asarray(tokens)\n",
        "\n",
        "def tokens_to_strokes(tokens):\n",
        "    \"\"\"\n",
        "    tokens is a 1d numpy array of stroke tokens to be converted into strokes\n",
        "    Returns a list of the strokes\n",
        "    \"\"\"\n",
        "    strokes = []\n",
        "    for token in tokens:\n",
        "        stroke = [0] * 5\n",
        "        if token // 1000000 == 1:\n",
        "            stroke[4] = 1\n",
        "            token -= 1000000\n",
        "        elif token // 500000 == 1:\n",
        "            stroke[3] = 1\n",
        "            token -= 500000\n",
        "        else:\n",
        "            stroke[2] = 1\n",
        "            token -= 100\n",
        "        stroke[0] = token // 511 - 255\n",
        "        stroke[1] = token % 511 - 255\n",
        "        strokes.append(stroke)\n",
        "    return np.asarray(strokes)\n",
        "\n",
        "\n",
        "def clean(data, max_length=100):\n",
        "    \"\"\"\n",
        "    Data is a np 3d array of samples in stroke-3 format\n",
        "    Removes all samples with length > max_length\n",
        "    Converts to stroke-5 and pads to max_length\n",
        "    Tokenizes stroke-5 vectors\n",
        "    Scales offsets down by a factor of eight and rounds\n",
        "    Returns tokenized dataset as a np 2d array\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "    for sample in data:\n",
        "        if len(sample) <= max_length:\n",
        "            sample = to_big_strokes(sample, max_length)\n",
        "            dataset.append(sample)\n",
        "    dataset = np.asarray(dataset)\n",
        "    #dataset[:, :, :2] /= 8\n",
        "    #dataset = np.round(dataset)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def offset_to_absolute(data):\n",
        "    \"\"\"\n",
        "    Converts stroke-5 format vectors from\n",
        "        [x_offset, y_offset, pen_down, pen_up, drawing_done]\n",
        "    to\n",
        "        [x_absolute, y_absolute, pen_down, pen_up, drawing_done]\n",
        "    \"\"\"\n",
        "    result = np.copy(data)\n",
        "    result[:, 0, :2] = [120, 68]\n",
        "    for i in range(1, data.shape[1]):\n",
        "        result[:, i, 0] += result[:, i - 1, 0]\n",
        "        result[:, i, 1] += result[:, i - 1, 1]\n",
        "\n",
        "    return result\n",
        "\n",
        "class Dataset:\n",
        "    \"\"\" Document later \"\"\"\n",
        "\n",
        "    def __init__(self, filepath, batch_size=32, max_length=250):\n",
        "        \"\"\" Init \"\"\"\n",
        "        data = np.load(\n",
        "            filepath,\n",
        "            encoding='latin1',\n",
        "            allow_pickle=True\n",
        "        )\n",
        "\n",
        "        # Clean up dataset, removing samples over max_length\n",
        "        # and tokenizing\n",
        "        self.train = clean(data['train'])\n",
        "        self.valid = clean(data['valid'])\n",
        "        self.test = clean(data['test'])\n",
        "\n",
        "        # Convert to tensorflow datasets for training\n",
        "        self.train = tf.convert_to_tensor(self.train)\n",
        "        self.train = tf.data.Dataset.from_tensor_slices(list(self.train))\n",
        "        self.valid = tf.convert_to_tensor(self.valid)\n",
        "        self.valid = tf.data.Dataset.from_tensor_slices(list(self.valid))\n",
        "        self.test = tf.convert_to_tensor(self.test)\n",
        "        self.test = tf.data.Dataset.from_tensor_slices(list(self.test))\n",
        "\n",
        "        # Shuffle and batch train and valid sets\n",
        "        self.train = self.train.shuffle(max_length)\n",
        "        self.valid = self.valid.shuffle(max_length)\n",
        "        self.train = self.train.batch(batch_size)\n",
        "        self.valid = self.valid.batch(batch_size)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgs8YYQyF2z9"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mygUDwq5IZY"
      },
      "source": [
        "# to do list for training\n",
        "# break dataset into batches\n",
        "# mask off parts for sets\n",
        "# create loss function\n",
        "\n",
        "# Dataset = __import__('3-dataset').Dataset\n",
        "# create_masks = __import__('4-create_masks').create_masks\n",
        "# Transformer = __import__('5-transformer').Transformer\n",
        "def train_transformer(Nb, No, Np, dm, h, hidden, max_len, batch_size, epochs):\n",
        "    \"\"\" creates and trains a transformer model for machine translation\n",
        "          of portuguese to English\n",
        "        Nb - number of blocks in the base of the model\n",
        "        No - number of blocks in the x/y offset branch\n",
        "        Np - number of blocks in the pen state prediction branch\n",
        "        dm: dimensionality of the model\n",
        "        h: number of heads\n",
        "        hidden: number of hidden units in fc layers\n",
        "        max_len: max tokens per sequence\n",
        "        batch_size: batch size for training\n",
        "        epochs: number of epochs\n",
        "        adam opt (beta1=0.9, beta2=0.98, epsilon=1e-9)\n",
        "        also special learning rat eequation\n",
        "        sparse categorical crossentropy\n",
        "        Returns: the trained model\n",
        "    \"\"\"\n",
        "\n",
        "    def create_labels(data):\n",
        "        \"\"\"\n",
        "        Creates labels for a stroke-5 format dataset\n",
        "        Each sample of size (?, 5) creates 3 one-hot labels of size (?, 481),\n",
        "        (?, 271), and (?, 3) for x offset, y offset, and pen state probabilities\n",
        "        respectively\n",
        "        Returns x_labels, y_labels, and pen_labels\n",
        "        \"\"\"\n",
        "        x_labels = np.zeros(data.shape[:2] + (481,))\n",
        "        y_labels = np.zeros(data.shape[:2] + (271,))\n",
        "\n",
        "        pen_labels = data[:, :, 2:]\n",
        "\n",
        "        for i, sample in enumerate(data[:16]):\n",
        "            for j, point in enumerate(sample):\n",
        "                x_labels[i, j, int(data[i, j, 0]) + 240] = 1\n",
        "                y_labels[i, j, int(data[i, j, 1]) + 135] = 1\n",
        "\n",
        "        x_labels = tf.convert_to_tensor(x_labels)\n",
        "        y_labels = tf.convert_to_tensor(y_labels)\n",
        "        pen_labels = tf.convert_to_tensor(pen_labels)\n",
        "\n",
        "        return [x_labels, y_labels, pen_labels]\n",
        "\n",
        "    def create_mask(batch_size, seq_len):\n",
        "        \"\"\"\n",
        "        Creates the look mask for attention in the decoder\n",
        "        seq_len: Length of the sequence for which to make the mask\n",
        "        \"\"\"\n",
        "        \n",
        "        mask = 1 - tf.linalg.band_part(tf.ones((1, 1, seq_len, seq_len)), -1, 0)\n",
        "        return mask\n",
        "\n",
        "\n",
        "    offset_loss_ = tf.keras.losses.MeanSquaredError()\n",
        "    pen_loss_ = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    def loss_function(real, offsets, pen_states):\n",
        "        \"\"\" custom loss function for transformer \n",
        "            real are real values of output\n",
        "            pred: output of the model\n",
        "        \"\"\"\n",
        "\n",
        "        offset_loss = offset_loss_(real[:, :, :2], offsets)\n",
        "        pen_loss = pen_loss_(real[:, :, 2:], pen_states)\n",
        "\n",
        "        return offset_loss, pen_loss\n",
        "\n",
        "      # can cast if necessary like so\n",
        "      # mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "      # loss_ *= mask\n",
        "      # do we still need to / by reduced sum\n",
        "\n",
        "    # create dataset\n",
        "    data = Dataset('cat.npz', batch_size=batch_size)\n",
        "    \n",
        "    # then create transformer\n",
        "    transformer = Decoder(Nb, No, Np, dm, h, hidden, max_len)\n",
        "\n",
        "    # Run a dummy set of inputs through to initialize weights\n",
        "    # and create lists of weights to apply gradients to\n",
        "    # Done to separate loss between offset and pen state branches\n",
        "    inputs = np.random.uniform(size=(1, 100, 5))\n",
        "    transformer(inputs, None)\n",
        "\n",
        "    offset_weights = []\n",
        "    pen_weights = []\n",
        "    for weight in transformer.trainable_weights:\n",
        "        if \"base\" in weight.name:\n",
        "            offset_weights.append(weight)\n",
        "            pen_weights.append(weight)\n",
        "        if \"offset\" in weight.name:\n",
        "            offset_weights.append(weight)\n",
        "        if \"pen\" in weight.name:\n",
        "            pen_weights.append(weight)\n",
        "\n",
        "    # set some hyper param stuff\n",
        "    pen_train_loss = tf.keras.metrics.Mean(name='pen_train_loss')\n",
        "    offset_train_loss = tf.keras.metrics.Mean(name='offset_train_loss')\n",
        "    learning_rate = CustomSchedule(dm)\n",
        "    #learning_rate = 0.001\n",
        "    offset_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
        "                                                name='offset optimizer')\n",
        "    pen_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
        "                                             name='pen_optimizer')\n",
        "    \n",
        "    def train_step(inputs):\n",
        "        \"\"\" single train step \n",
        "            takes in input and a target value\n",
        "        \"\"\"\n",
        "        mask = create_mask(batch_size, max_len - 1)\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            offsets, pen_states = transformer(inputs[:, :-1], mask, True)\n",
        "\n",
        "            # must also take third arg for this loss f'n\n",
        "            # loss = loss_function(tar_real, predictions)\n",
        "            offset_loss, pen_loss = loss_function(inputs[:, 1:], offsets, pen_states)\n",
        "\n",
        "        grads = tape.gradient(offset_loss, offset_weights)\n",
        "        offset_optimizer.apply_gradients(zip(grads, offset_weights))\n",
        "        grads = tape.gradient(pen_loss, pen_weights)\n",
        "        pen_optimizer.apply_gradients(zip(grads, pen_weights))\n",
        "        offset_train_loss(offset_loss)\n",
        "        pen_train_loss(pen_loss)\n",
        "        del tape\n",
        "\n",
        "    offset_losses = []\n",
        "    pen_losses = []\n",
        "\n",
        "    # then do the training\n",
        "    for epoch in range(epochs):\n",
        "        offset_train_loss.reset_states()\n",
        "        pen_train_loss.reset_states()\n",
        "        for batch, inp in enumerate(data.train):\n",
        "            train_step(inp)\n",
        "\n",
        "            offset_losses.append(offset_train_loss.result())\n",
        "            pen_losses.append(pen_train_loss.result())\n",
        "\n",
        "            if batch % 50 == 0:\n",
        "                if batch % 50 == 0:\n",
        "                    print(\"Epoch {}, batch {}: Offset Loss: {} Pen Loss {}\".format(\n",
        "                        epoch + 1,\n",
        "                        batch,\n",
        "                        offset_train_loss.result(),\n",
        "                        pen_train_loss.result()\n",
        "                    ))\n",
        "        print(\"Epoch {}: Offset Loss: {} Pen Loss {}\".format(\n",
        "            epoch + 1,\n",
        "            offset_train_loss.result(),\n",
        "            pen_train_loss.result()\n",
        "        ))\n",
        "        transformer.save_weights('epoch_' + str(epoch))\n",
        "\n",
        "        if epoch % 5 == 0 and epoch > 0:\n",
        "            learning_rate /= 2\n",
        "\n",
        "    return transformer, offset_losses, pen_losses\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\" Custom learning rate schedule \"\"\"\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        \"\"\" Init \"\"\"\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        \"\"\" Call \"\"\"\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8ITDUUs08L6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2325f42-ffaf-41bf-de78-781cd1ded8a5"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "# from transformer import Encoder\n",
        "# from utils import to_big_strokes\n",
        "# from dataset import Dataset\n",
        "\n",
        "model, offset_losses, pen_losses = train_transformer(4, 2, 2, 256, 8, 512, 100, 64, 3)\n",
        "\n",
        "offset_losses = [tf.reduce_sum(loss).numpy() for loss in offset_losses]\n",
        "\n",
        "plt.plot(range(len(offset_losses)), offset_losses)\n",
        "plt.show()\n",
        "\n",
        "model.save_weights('branched_5_epoch.h5')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, batch 0: Offset Loss: 1149.0440673828125 Pen Loss 1.4293053150177002\n",
            "Epoch 1, batch 50: Offset Loss: 1137.49267578125 Pen Loss 1.0188302993774414\n",
            "Epoch 1, batch 100: Offset Loss: 1122.8873291015625 Pen Loss 0.7536736130714417\n",
            "Epoch 1, batch 150: Offset Loss: 1096.431396484375 Pen Loss 0.6182723045349121\n",
            "Epoch 1, batch 200: Offset Loss: 1070.9224853515625 Pen Loss 0.543330729007721\n",
            "Epoch 1, batch 250: Offset Loss: 1053.44482421875 Pen Loss 0.49598929286003113\n",
            "Epoch 1, batch 300: Offset Loss: 1042.2706298828125 Pen Loss 0.46370425820350647\n",
            "Epoch 1, batch 350: Offset Loss: 1032.1123046875 Pen Loss 0.4403620958328247\n",
            "Epoch 1, batch 400: Offset Loss: 1018.4906005859375 Pen Loss 0.421571284532547\n",
            "Epoch 1, batch 450: Offset Loss: 1009.6517944335938 Pen Loss 0.40669819712638855\n",
            "Epoch 1, batch 500: Offset Loss: 1000.5267944335938 Pen Loss 0.39469626545906067\n",
            "Epoch 1, batch 550: Offset Loss: 993.4723510742188 Pen Loss 0.38477960228919983\n",
            "Epoch 1, batch 600: Offset Loss: 986.48095703125 Pen Loss 0.3764301538467407\n",
            "Epoch 1, batch 650: Offset Loss: 979.0904541015625 Pen Loss 0.3691331148147583\n",
            "Epoch 1, batch 700: Offset Loss: 972.8270263671875 Pen Loss 0.3628864884376526\n",
            "Epoch 1, batch 750: Offset Loss: 967.7822265625 Pen Loss 0.3574618101119995\n",
            "Epoch 1, batch 800: Offset Loss: 963.8135986328125 Pen Loss 0.35256484150886536\n",
            "Epoch 1, batch 850: Offset Loss: 959.1419067382812 Pen Loss 0.3483229875564575\n",
            "Epoch 1, batch 900: Offset Loss: 954.3438110351562 Pen Loss 0.34441107511520386\n",
            "Epoch 1, batch 950: Offset Loss: 952.2046508789062 Pen Loss 0.3410528004169464\n",
            "Epoch 1: Offset Loss: 949.8552856445312 Pen Loss 0.33933380246162415\n",
            "Epoch 2, batch 0: Offset Loss: 892.3494262695312 Pen Loss 0.2887625992298126\n",
            "Epoch 2, batch 50: Offset Loss: 905.091064453125 Pen Loss 0.27524304389953613\n",
            "Epoch 2, batch 100: Offset Loss: 900.7265014648438 Pen Loss 0.27624544501304626\n",
            "Epoch 2, batch 150: Offset Loss: 889.226806640625 Pen Loss 0.2755894958972931\n",
            "Epoch 2, batch 200: Offset Loss: 884.4260864257812 Pen Loss 0.27423566579818726\n",
            "Epoch 2, batch 250: Offset Loss: 880.672607421875 Pen Loss 0.2737424075603485\n",
            "Epoch 2, batch 300: Offset Loss: 882.7640380859375 Pen Loss 0.27398356795310974\n",
            "Epoch 2, batch 350: Offset Loss: 881.3706665039062 Pen Loss 0.2744619846343994\n",
            "Epoch 2, batch 400: Offset Loss: 875.1246948242188 Pen Loss 0.2738719880580902\n",
            "Epoch 2, batch 450: Offset Loss: 874.7017211914062 Pen Loss 0.27350470423698425\n",
            "Epoch 2, batch 500: Offset Loss: 871.7971801757812 Pen Loss 0.27310100197792053\n",
            "Epoch 2, batch 550: Offset Loss: 869.343017578125 Pen Loss 0.2728569507598877\n",
            "Epoch 2, batch 600: Offset Loss: 866.8546752929688 Pen Loss 0.2726675271987915\n",
            "Epoch 2, batch 650: Offset Loss: 864.0217895507812 Pen Loss 0.272328644990921\n",
            "Epoch 2, batch 700: Offset Loss: 861.1766967773438 Pen Loss 0.27190420031547546\n",
            "Epoch 2, batch 750: Offset Loss: 857.2923583984375 Pen Loss 0.2715360224246979\n",
            "Epoch 2, batch 800: Offset Loss: 855.7181396484375 Pen Loss 0.2708321511745453\n",
            "Epoch 2, batch 850: Offset Loss: 851.2510375976562 Pen Loss 0.2700210213661194\n",
            "Epoch 2, batch 900: Offset Loss: 845.829345703125 Pen Loss 0.2690921425819397\n",
            "Epoch 2, batch 950: Offset Loss: 842.107177734375 Pen Loss 0.2681448459625244\n",
            "Epoch 2: Offset Loss: 839.0365600585938 Pen Loss 0.2675408124923706\n",
            "Epoch 3, batch 0: Offset Loss: 899.1028442382812 Pen Loss 0.26917093992233276\n",
            "Epoch 3, batch 50: Offset Loss: 730.5724487304688 Pen Loss 0.24515926837921143\n",
            "Epoch 3, batch 100: Offset Loss: 734.8368530273438 Pen Loss 0.24397391080856323\n",
            "Epoch 3, batch 150: Offset Loss: 722.0798950195312 Pen Loss 0.2428143471479416\n",
            "Epoch 3, batch 200: Offset Loss: 714.4395141601562 Pen Loss 0.2411939948797226\n",
            "Epoch 3, batch 250: Offset Loss: 708.8693237304688 Pen Loss 0.24029816687107086\n",
            "Epoch 3, batch 300: Offset Loss: 703.22412109375 Pen Loss 0.2393619567155838\n",
            "Epoch 3, batch 350: Offset Loss: 694.402099609375 Pen Loss 0.23882226645946503\n",
            "Epoch 3, batch 400: Offset Loss: 685.6231079101562 Pen Loss 0.23752367496490479\n",
            "Epoch 3, batch 450: Offset Loss: 679.8889770507812 Pen Loss 0.23650915920734406\n",
            "Epoch 3, batch 500: Offset Loss: 674.8359985351562 Pen Loss 0.23570516705513\n",
            "Epoch 3, batch 550: Offset Loss: 669.0197143554688 Pen Loss 0.23493987321853638\n",
            "Epoch 3, batch 600: Offset Loss: 663.554931640625 Pen Loss 0.23425528407096863\n",
            "Epoch 3, batch 650: Offset Loss: 659.0462036132812 Pen Loss 0.2334129959344864\n",
            "Epoch 3, batch 700: Offset Loss: 654.7205810546875 Pen Loss 0.23274698853492737\n",
            "Epoch 3, batch 750: Offset Loss: 650.6119995117188 Pen Loss 0.2321123331785202\n",
            "Epoch 3, batch 800: Offset Loss: 648.95751953125 Pen Loss 0.23173989355564117\n",
            "Epoch 3, batch 850: Offset Loss: 645.5140380859375 Pen Loss 0.23135437071323395\n",
            "Epoch 3, batch 900: Offset Loss: 642.394775390625 Pen Loss 0.2309485673904419\n",
            "Epoch 3, batch 950: Offset Loss: 641.3779296875 Pen Loss 0.23064641654491425\n",
            "Epoch 3: Offset Loss: 639.3671875 Pen Loss 0.230458602309227\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7HcIUSTV6uh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01468aab-f8b0-4b03-e59a-ed18844e7933"
      },
      "source": [
        "### this is our test file for fisplay after training\n",
        "# test = data['train'][4]\n",
        "\n",
        "def offset_to_absolute(data):\n",
        "    \"\"\"\n",
        "    Converts stroke-5 format vectors from\n",
        "        [x_offset, y_offset, pen_down, pen_up, drawing_done]\n",
        "    to\n",
        "        [x_absolute, y_absolute, pen_down, pen_up, drawing_done]\n",
        "    \"\"\"\n",
        "    result = np.copy(data)\n",
        "    result[:, 0, :2] = [120, 68]\n",
        "    for i in range(1, data.shape[1]):\n",
        "        result[:, i, 0] += result[:, i - 1, 0]\n",
        "        result[:, i, 1] += result[:, i - 1, 1]\n",
        "\n",
        "    return result\n",
        "\n",
        "def to_big_strokes(stroke, max_len=100):\n",
        "    \"\"\"Converts from stroke-3 to stroke-5 format and pads to given length.\"\"\"\n",
        "    # (But does not insert special start token).\n",
        "\n",
        "    result = np.zeros((max_len, 5), dtype=float)\n",
        "    l = len(stroke)\n",
        "    assert l <= max_len\n",
        "    result[0:l, 0:2] = stroke[:, 0:2]\n",
        "    result[0:l, 3] = stroke[:, 2]\n",
        "    result[0:l, 2] = 1 - result[0:l, 3]\n",
        "    result[l:, 4] = 1\n",
        "    return result\n",
        "\n",
        "\n",
        "def create_mask(batch_size, seq_len):\n",
        "        \"\"\"\n",
        "        Creates the look mask for attention in the decoder\n",
        "        seq_len: Length of the sequence for which to make the mask\n",
        "        \"\"\"\n",
        "        \n",
        "        mask = 1 - tf.linalg.band_part(tf.ones((1, 1, seq_len, seq_len)), -1, 0)\n",
        "        return mask\n",
        "\n",
        "def create_labels(data):\n",
        "    \"\"\"\n",
        "    Creates labels for a stroke-5 format dataset\n",
        "    Each sample of size (?, 5) creates 3 one-hot labels of size (?, 481),\n",
        "    (?, 271), and (?, 3) for x offset, y offset, and pen state probabilities\n",
        "    respectively\n",
        "    Returns x_labels, y_labels, and pen_labels\n",
        "    \"\"\"\n",
        "    x_labels = np.zeros(data.shape[:2] + (481,))\n",
        "    y_labels = np.zeros(data.shape[:2] + (271,))\n",
        "\n",
        "    pen_labels = data[:, :, 2:]\n",
        "\n",
        "    for i, sample in enumerate(data):\n",
        "        for j, point in enumerate(sample):\n",
        "            x_labels[i, j, int(data[i, j, 0]) + 240] = 1\n",
        "            y_labels[i, j, int(data[i, j, 1]) + 135] = 1\n",
        "\n",
        "    x_labels = tf.convert_to_tensor(x_labels)\n",
        "    y_labels = tf.convert_to_tensor(y_labels)\n",
        "    pen_labels = tf.convert_to_tensor(pen_labels)\n",
        "\n",
        "    return [x_labels, y_labels, pen_labels]\n",
        "\n",
        "\n",
        "data = np.load('cat.npz', encoding='latin1', allow_pickle=True)\n",
        "test = data['test'][4]\n",
        "test = to_big_strokes(test)\n",
        "test[:, :2] = test[:, :2]\n",
        "inputs = test[np.newaxis, :18]\n",
        "\n",
        "while inputs[0, -1, -1] != 1 and inputs.shape[1] < 100:\n",
        "    mask = create_mask(1, inputs.shape[1])\n",
        "    offsets, pen_states = model(inputs, mask)\n",
        "    offsets = np.round(offsets)\n",
        "    pen_states = np.round(pen_states)\n",
        "    pred = np.concatenate((offsets[0, -1], pen_states[0, -1]))\n",
        "    inputs = np.concatenate((inputs, pred.reshape(1, 1, 5)), axis=1)\n",
        "\n",
        "print(inputs[0])\n",
        "\n",
        "draw_strokes(test, svg_filename='realcat.svg')\n",
        "draw_strokes(inputs[0], svg_filename=\"cat.svg\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-18.   5.   1.   0.   0.]\n",
            " [-15.  16.   1.   0.   0.]\n",
            " [ -7.  13.   1.   0.   0.]\n",
            " [ -1.  36.   1.   0.   0.]\n",
            " [  9.  15.   1.   0.   0.]\n",
            " [ 28.  26.   1.   0.   0.]\n",
            " [ 13.   4.   1.   0.   0.]\n",
            " [ 29.  -1.   1.   0.   0.]\n",
            " [ 32. -15.   1.   0.   0.]\n",
            " [ 24. -22.   1.   0.   0.]\n",
            " [ 11. -16.   1.   0.   0.]\n",
            " [  3. -10.   1.   0.   0.]\n",
            " [ -2. -23.   1.   0.   0.]\n",
            " [-12. -18.   1.   0.   0.]\n",
            " [-24. -12.   1.   0.   0.]\n",
            " [-21.  -3.   1.   0.   0.]\n",
            " [-48.   0.   1.   0.   0.]\n",
            " [-19.   9.   0.   1.   0.]\n",
            " [ 11.   9.   1.   0.   0.]\n",
            " [  2. -10.   1.   0.   0.]\n",
            " [  0. -24.   1.   0.   0.]\n",
            " [  8.  -3.   1.   0.   0.]\n",
            " [ 17.  12.   1.   0.   0.]\n",
            " [ 21.  22.   1.   0.   0.]\n",
            " [ 15.  11.   0.   1.   0.]\n",
            " [ 38.   0.   1.   0.   0.]\n",
            " [ 22. -18.   1.   0.   0.]\n",
            " [  6.   3.   1.   0.   0.]\n",
            " [ -3.  25.   1.   0.   0.]\n",
            " [ -9.  28.   0.   1.   0.]\n",
            " [-83.  30.   1.   0.   0.]\n",
            " [ -2.   8.   1.   0.   0.]\n",
            " [  2.   5.   1.   0.   0.]\n",
            " [  4.   1.   1.   0.   0.]\n",
            " [  3.  -3.   1.   0.   0.]\n",
            " [ -1.  -5.   1.   0.   0.]\n",
            " [ -3.  -1.   1.   0.   0.]\n",
            " [ -3.   3.   0.   1.   0.]\n",
            " [ 30.   3.   1.   0.   0.]\n",
            " [ -1.   2.   1.   0.   0.]\n",
            " [ -2.   3.   1.   0.   0.]\n",
            " [ -1.   3.   1.   0.   0.]\n",
            " [ -1.   3.   1.   0.   0.]\n",
            " [ -0.   3.   1.   0.   0.]\n",
            " [  1.   2.   1.   0.   0.]\n",
            " [  1.   0.   1.   0.   0.]\n",
            " [ -2.  -3.   1.   0.   0.]\n",
            " [ -3.  -0.   0.   1.   0.]\n",
            " [-23.  14.   1.   0.   0.]\n",
            " [ -1.   3.   1.   0.   0.]\n",
            " [  1.   3.   1.   0.   0.]\n",
            " [  3.   2.   1.   0.   0.]\n",
            " [  4.   0.   1.   0.   0.]\n",
            " [  4.  -2.   1.   0.   0.]\n",
            " [  2.  -3.   0.   1.   0.]\n",
            " [-11.   6.   1.   0.   0.]\n",
            " [ -1.   3.   1.   0.   0.]\n",
            " [  1.   3.   1.   0.   0.]\n",
            " [  3.   2.   1.   0.   0.]\n",
            " [  4.  -0.   1.   0.   0.]\n",
            " [  4.  -2.   0.   1.   0.]\n",
            " [ -8.   2.   1.   0.   0.]\n",
            " [ -1.   2.   0.   1.   0.]\n",
            " [ -1.   4.   1.   0.   0.]\n",
            " [  2.   2.   1.   0.   0.]\n",
            " [  4.   0.   0.   1.   0.]\n",
            " [ -6.   1.   1.   0.   0.]\n",
            " [ -1.   2.   0.   1.   0.]\n",
            " [ -1.   3.   1.   0.   0.]\n",
            " [  1.   2.   0.   1.   0.]\n",
            " [ -3.   2.   1.   0.   0.]\n",
            " [ -0.   2.   0.   1.   0.]\n",
            " [ -2.   1.   1.   0.   0.]\n",
            " [  0.   1.   0.   1.   0.]\n",
            " [ -1.   1.   1.   0.   0.]\n",
            " [  1.   1.   0.   1.   0.]\n",
            " [ -3.   1.   1.   0.   0.]\n",
            " [ -0.   2.   0.   1.   0.]\n",
            " [ -1.   1.   1.   0.   0.]\n",
            " [  1.   2.   0.   1.   0.]\n",
            " [ -3.   0.   0.   0.   1.]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"1095.0\" version=\"1.1\" width=\"795.0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"1095.0\" width=\"795.0\" x=\"0\" y=\"0\"/><path d=\"M230.0,495.0 m-90.0,25.0 l-75.0,80.0 -35.0,65.0 l-5.0,180.0 45.0,75.0 l140.0,130.0 65.0,20.0 l145.0,-5.0 160.0,-75.0 l120.0,-110.0 55.0,-80.0 l15.0,-50.0 -10.0,-115.0 l-60.0,-90.0 -120.0,-60.0 l-105.0,-15.0 -240.0,0.0 l-95.0,45.0 m-70.0,5.0 l45.0,-175.0 25.0,-45.0 l15.0,-155.0 40.0,-120.0 l25.0,35.0 30.0,195.0 l95.0,195.0 m175.0,25.0 l90.0,-225.0 40.0,-155.0 l25.0,5.0 40.0,225.0 l0.0,110.0 -25.0,95.0 m-500.0,245.0 l15.0,65.0 25.0,30.0 l115.0,-20.0 30.0,-45.0 l0.0,35.0 20.0,25.0 l55.0,30.0 25.0,0.0 l20.0,-25.0 0.0,-65.0 m-295.0,-190.0 l30.0,100.0 m185.0,-105.0 l10.0,140.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"1040.0\" version=\"1.1\" width=\"865.0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"1040.0\" width=\"865.0\" x=\"0\" y=\"0\"/><path d=\"M230.0,145.0 m-90.0,25.0 l-75.0,80.0 -35.0,65.0 l-5.0,180.0 45.0,75.0 l140.0,130.0 65.0,20.0 l145.0,-5.0 160.0,-75.0 l120.0,-110.0 55.0,-80.0 l15.0,-50.0 -10.0,-115.0 l-60.0,-90.0 -120.0,-60.0 l-105.0,-15.0 -240.0,0.0 l-95.0,45.0 m55.0,45.0 l10.0,-50.0 0.0,-120.0 l40.0,-15.0 85.0,60.0 l105.0,110.0 75.0,55.0 m190.0,0.0 l110.0,-90.0 30.0,15.0 l-15.0,125.0 -45.0,140.0 m-415.0,150.0 l-10.0,40.0 10.0,25.0 l20.0,5.0 15.0,-15.0 l-5.0,-25.0 -15.0,-5.0 l-15.0,15.0 m150.0,15.0 l-5.0,10.0 -10.0,15.0 l-5.0,15.0 -5.0,15.0 l-0.0,15.0 5.0,10.0 l5.0,0.0 -10.0,-15.0 l-15.0,-0.0 m-115.0,70.0 l-5.0,15.0 5.0,15.0 l15.0,10.0 20.0,0.0 l20.0,-10.0 10.0,-15.0 m-55.0,30.0 l-5.0,15.0 5.0,15.0 l15.0,10.0 20.0,-0.0 l20.0,-10.0 m-40.0,10.0 l-5.0,10.0 m-5.0,20.0 l10.0,10.0 20.0,0.0 m-30.0,5.0 l-5.0,10.0 m-5.0,15.0 l5.0,10.0 m-15.0,10.0 l-0.0,10.0 m-10.0,5.0 l0.0,5.0 m-5.0,5.0 l5.0,5.0 m-15.0,5.0 l-0.0,10.0 m-5.0,5.0 l5.0,10.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP0VicY76HbK"
      },
      "source": [
        "# drawing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74E7ysgRZ2sc"
      },
      "source": [
        "# libraries required for visualisation:\n",
        "import os\n",
        "import svgwrite\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from IPython.display import SVG, display\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "# import data_Manager\n",
        "import math\n",
        "from matplotlib import animation\n",
        "\n",
        "# data_Manager = Data\n",
        "# set numpy output to something sensible\n",
        "np.set_printoptions(precision=8, edgeitems=6, linewidth=200, suppress=True)\n",
        "\n",
        "\n",
        "def get_bounds(data, factor=10):\n",
        "    \"\"\"Return bounds of data.\"\"\"\n",
        "    min_x = 0\n",
        "    max_x = 0\n",
        "    min_y = 0\n",
        "    max_y = 0\n",
        "\n",
        "    abs_x = 0\n",
        "    abs_y = 0\n",
        "    for i in range(len(data)):\n",
        "        x = float(data[i, 0]) / factor\n",
        "        y = float(data[i, 1]) / factor\n",
        "        abs_x += x\n",
        "        abs_y += y\n",
        "        min_x = min(min_x, abs_x)\n",
        "        min_y = min(min_y, abs_y)\n",
        "        max_x = max(max_x, abs_x)\n",
        "        max_y = max(max_y, abs_y)\n",
        "\n",
        "    return (min_x, max_x, min_y, max_y)\n",
        "\n",
        "\n",
        "def slerp(p0, p1, t):\n",
        "    \"\"\"Spherical interpolation.\"\"\"\n",
        "    omega = np.arccos(np.dot(p0 / np.linalg.norm(p0), p1 / np.linalg.norm(p1)))\n",
        "    so = np.sin(omega)\n",
        "    return np.sin((1.0 - t) * omega) / so * p0 + np.sin(t * omega) / so * p1\n",
        "\n",
        "\n",
        "def lerp(p0, p1, t):\n",
        "    \"\"\"Linear interpolation.\"\"\"\n",
        "    return (1.0 - t) * p0 + t * p1\n",
        "\n",
        "\n",
        "def to_normal_strokes(big_stroke):\n",
        "    \"\"\"Convert from stroke-5 format to stroke-3.\"\"\"\n",
        "    l = 0\n",
        "    for i in range(len(big_stroke)):\n",
        "        if big_stroke[i, 4] > 0:\n",
        "            l = i\n",
        "            break\n",
        "    if l == 0:\n",
        "        l = len(big_stroke)\n",
        "    result = np.zeros((l, 3))\n",
        "    result[:, 0:2] = big_stroke[0:l, 0:2]\n",
        "    result[:, 2] = big_stroke[0:l, 3]\n",
        "    return result\n",
        "\n",
        "\n",
        "# little function that displays vector images and saves them to .svg\n",
        "def draw_strokes(data, factor=0.2, svg_filename = '/tmp/sketch_rnn/svg/sample.svg'):\n",
        "    # data = data_Manager.to_normal_strokes(data)\n",
        "    data = to_normal_strokes(data)\n",
        "    min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
        "    dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
        "    dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
        "    dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n",
        "    lift_pen = 1\n",
        "    abs_x = 25 - min_x \n",
        "    abs_y = 25 - min_y\n",
        "    p = \"M%s,%s \" % (abs_x, abs_y)\n",
        "    command = \"m\"\n",
        "    for i in range(len(data)):\n",
        "        if (lift_pen == 1):\n",
        "            command = \"m\"\n",
        "        elif (command != \"l\"):\n",
        "            command = \"l\"\n",
        "        else:\n",
        "            command = \"\"\n",
        "        x = float(data[i,0])/factor\n",
        "        y = float(data[i,1])/factor\n",
        "        lift_pen = data[i, 2]\n",
        "        p += command+str(x)+\",\"+str(y)+\" \"\n",
        "    the_color = \"black\"\n",
        "    stroke_width = 1\n",
        "    dwg.add(dwg.path(p).stroke(the_color,stroke_width).fill(\"none\"))\n",
        "    dwg.save()\n",
        "    display(SVG(dwg.tostring()))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Function for animate drawing. \n",
        "taken from \n",
        "https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/Strokes_QuickDraw.ipynb#scrollTo=0ABX6O4kYwYS\n",
        "\"\"\"\n",
        "def create_animation(drawing, fps = 30, idx = 0, lw = 5): \n",
        "  \n",
        "  seq_length = 0 \n",
        "  \n",
        "  xmax = 0 \n",
        "  ymax = 0 \n",
        "  \n",
        "  xmin = math.inf\n",
        "  ymin = math.inf\n",
        "  \n",
        "  #retreive min,max and the length of the drawing  \n",
        "  for k in range(0, len(drawing)):\n",
        "    x = drawing[k][0]\n",
        "    y = drawing[k][1]\n",
        "\n",
        "    seq_length += len(x)\n",
        "    xmax = max([max(x), xmax]) \n",
        "    ymax = max([max(y), ymax]) \n",
        "    \n",
        "    xmin = min([min(x), xmin]) \n",
        "    ymin = min([min(y), ymin]) \n",
        "    \n",
        "  i = 0 \n",
        "  j = 0\n",
        "  \n",
        "  # First set up the figure, the axis, and the plot element we want to animate\n",
        "  fig = plt.figure()\n",
        "  ax = plt.axes(xlim=(xmax+lw, xmin-lw), ylim=(ymax+lw, ymin-lw))\n",
        "  ax.set_facecolor(\"white\")\n",
        "  line, = ax.plot([], [], lw=lw)\n",
        "\n",
        "  #remove the axis \n",
        "  ax.grid = False\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  \n",
        "  # initialization function: plot the background of each frame\n",
        "  def init():\n",
        "      line.set_data([], [])\n",
        "      return line, \n",
        "\n",
        "  # animation function.  This is called sequentially\n",
        "  def animate(frame):    \n",
        "    nonlocal i, j, line\n",
        "    x = drawing[i][0]\n",
        "    y = drawing[i][1]\n",
        "    line.set_data(x[0:j], y[0:j])\n",
        "    \n",
        "    if j >= len(x):\n",
        "      i +=1\n",
        "      j = 0 \n",
        "      line, = ax.plot([], [], lw=lw)\n",
        "      \n",
        "    else:\n",
        "      j += 1\n",
        "    return line,\n",
        "  \n",
        "  # call the animator.  blit=True means only re-draw the parts that have changed.\n",
        "  anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                                 frames= seq_length + len(drawing), blit=True)\n",
        "  plt.close()\n",
        "  \n",
        "  # save the animation as an mp4.  \n",
        "  anim.save(f'video.mp4', fps=fps, extra_args=['-vcodec', 'libx264'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKNGUZRvaBHT"
      },
      "source": [
        "import urllib.request\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "# from HyperParameters import HP\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "\n",
        "HP = hyper_parameters\n",
        "class Data():\n",
        "    # managing the data\n",
        "    def __init__(self, size = None):\n",
        "        'Initialization'\n",
        "        # first load the data\n",
        "        self.loadData(size)\n",
        "        self.dim = HP.input_dimention\n",
        "        self.batch_size = HP.batch_size\n",
        "        self.train = purify(self.train)\n",
        "        self.valid = purify(self.valid)\n",
        "        self.test = purify(self.test)\n",
        "        self.train = self.normalize(self.train)\n",
        "        self.valid = self.normalize(self.valid)\n",
        "        self.test = self.normalize(self.test)\n",
        "        self.train = np.array(self.train)\n",
        "        self.valid = np.array(self.valid)\n",
        "        self.test = np.array(self.test)\n",
        "    \n",
        "\n",
        "    def loadData(self, size):  \n",
        "        npzFile = np.load(HP.data_location, allow_pickle=True, encoding='latin1')\n",
        "        train = npzFile['train']\n",
        "        if size:\n",
        "            self.train = np.copy(train[:size])\n",
        "        else:\n",
        "            self.train = train\n",
        "        self.trainDimention = len(self.train)\n",
        "        \n",
        "        self.test = npzFile['test']\n",
        "        \n",
        "        self.valid = npzFile['valid']\n",
        "        self.validationDimention = len(self.valid)\n",
        "        return self.train, self.valid, self.test\n",
        "\n",
        "    def calculate_normalizing_scale_factor(self, strokes):\n",
        "        \"\"\"Calculate the normalizing factor explained in appendix of sketch-rnn.\"\"\"\n",
        "        data = []\n",
        "        for i in range(len(strokes)):\n",
        "            if len(strokes[i]) > HP.max_seq_length:\n",
        "                continue\n",
        "            for j in range(len(strokes[i])):\n",
        "                data.append(strokes[i][j, 0])\n",
        "                data.append(strokes[i][j, 1])\n",
        "        data = np.array(data)\n",
        "        return np.std(data)\n",
        "\n",
        "    def normalize(self, strokes,  scale_factor=None):\n",
        "        \"\"\"Normalize entire dataset (delta_x, delta_y) by the scaling factor.\"\"\"\n",
        "        if scale_factor is None:\n",
        "            scale_factor = self.calculate_normalizing_scale_factor(strokes)\n",
        "        self.scale_factor = scale_factor\n",
        "        for i in range(len(strokes)):\n",
        "            strokes[i][:, 0:2] /= self.scale_factor\n",
        "        return strokes\n",
        "\n",
        "def purify(strokes):\n",
        "    # We have to remove too long sequence \n",
        "    data = []\n",
        "    for seq in strokes:\n",
        "        if seq.shape[0] <= HP.max_seq_length:\n",
        "            len_seq = len(seq[:,0])\n",
        "            # pen state made by 3 state\n",
        "            new_seq = np.zeros((HP.max_seq_length,5))\n",
        "            new_seq[:len_seq,:2] = seq[:,:2]\n",
        "            new_seq[:len_seq-1,2] = 1-seq[:-1,2]\n",
        "            new_seq[:len_seq,3] = seq[:,2]\n",
        "            new_seq[len_seq:,4] = 1\n",
        "            data.append(new_seq)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def to_normal_strokes(big_stroke):\n",
        "    \"\"\"Convert from stroke-5 format to stroke-3.\"\"\"\n",
        "    l = 0\n",
        "    for i in range(len(big_stroke)):\n",
        "        if big_stroke[i, 4] > 0:\n",
        "            l = i\n",
        "            break\n",
        "    if l == 0:\n",
        "        l = len(big_stroke)\n",
        "    result = np.zeros((l, 3))\n",
        "    result[:, 0:2] = big_stroke[0:l, 0:2]\n",
        "    result[:, 2] = big_stroke[0:l, 3]\n",
        "    return result\n",
        "\n",
        "# see https://keras.io/utils/ for more info\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, Data, shuffle=True, validation = False):\n",
        "        'Initialization'\n",
        "        self.Data = Data\n",
        "        self.validation = validation\n",
        "        if validation:\n",
        "            self.batch_size = len(Data)\n",
        "        else:\n",
        "            self.batch_size = HP.batch_size\n",
        "        self.dimention = len(Data)\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        if self.validation:\n",
        "            return 1    \n",
        "        return int(np.floor(self.dimention/ self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        encoder_input = self.Data[indexes]\n",
        "        #if not self.validation:\n",
        "            #encoder_input = self.dataAugmentation(encoder_input)\n",
        "        decoder_ipnut = create_decoder_input(encoder_input)\n",
        "        \"\"\"\n",
        "        encoder_ipnut_short = []\n",
        "        for i in range(encoder_input.shape[0]):\n",
        "            for j in range(encoder_input.shape[1]):\n",
        "                if encoder_input[i,j,4] == 1:\n",
        "                    encoder_ipnut_short.append(np.copy(encoder_input[i, :j, :]))\n",
        "                    break\n",
        "        \"\"\"\n",
        "        \n",
        "        return [encoder_input, decoder_ipnut], [encoder_input]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(self.dimention)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def dataAugmentation(self, strokes):\n",
        "        # generate random uniform between 0.9 to 1.1\n",
        "        randomx = np.random.rand()*(0.1)+1.\n",
        "        randomy = np.random.rand()*(0.1)+1.\n",
        "        # multiply the \n",
        "        strokes[:,:,0] = strokes[:,:,0]*randomx\n",
        "        strokes[:,:,1] = strokes[:,:,1]*randomy\n",
        "        return strokes     \n",
        "\n",
        "def create_decoder_input(sequence):\n",
        "    \"\"\"\n",
        "    function that, given an input sequence returns another sequence \n",
        "    for the decoder. \n",
        "    It shift the old sequence by one and insert on head the value\n",
        "    (0,0,1,0,0)\n",
        "    \"\"\"\n",
        "    decoder_ipnut = np.zeros(shape=sequence.shape)\n",
        "    # copy the value of the sequence\n",
        "    decoder_ipnut[:,1:] = sequence[:,:-1]\n",
        "    decoder_ipnut[:,0] = np.array([0,0,1,0,0])\n",
        "\n",
        "    return decoder_ipnut\n",
        "        \n",
        "\n",
        "class changing_KL_wheight(Callback):\n",
        "    def __init__(self, kl_weight, verbose = 1, mu_min = 0.01):\n",
        "        super(Callback, self).__init__()\n",
        "        self.kl_wheight = kl_weight\n",
        "        self.verbose = verbose\n",
        "        self.curr_mu = 0\n",
        "\n",
        "    def on_epoch_begin(self, epochs, logs = {}):\n",
        "        self.curr_mu = 1 - (1-HP.eta_min)*HP.R**epochs\n",
        "        New_wheight_kl = (self.curr_mu)*HP.wKL\n",
        "        # IF I USE TF-2.0 then I have to update the variable like that\n",
        "        self.kl_wheight.assign(New_wheight_kl)\n",
        "        #tf.keras.backend.set_value(self.kl_wheight, New_wheight_kl)\n",
        "\n",
        "\n",
        "    def on_train_batch_begin(self, epochs, logs = {}):\n",
        "        pass\n",
        "\n",
        "    def on_train_batch_end(self, epochs, logs = {}):\n",
        "        pass\n",
        "\n",
        "    def on_test_batch_begin(self, epochs, logs = {}):\n",
        "        pass\n",
        "\n",
        "    def on_test_batch_end(self, epochs, logs = {}):\n",
        "        pass\n",
        "    \n",
        "    def on_test_begin(self, *arg, **karg):\n",
        "        pass\n",
        "\n",
        "    def on_test_end(self, *arg, **karg):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg9hfXYUaH49"
      },
      "source": [
        "class hyper_parameters():\n",
        "    def __init__(self):\n",
        "        # model location fodler\n",
        "        self.model_folder = \"model/\"\n",
        "        self.model_name = \"sketch-RNN.h5\"\n",
        "        # Location of the data and name of the file\n",
        "        # name = sketchrnn_airplane.full.npz\n",
        "        self.data_folder = \"data/\"\n",
        "        self.data_name = \"cat.npz\"\n",
        "        self.data_location = self.data_folder+self.data_name\n",
        "        # NN parameters\n",
        "        self.latent_dim = 256 \n",
        "        self.input_dimention = 5\n",
        "        self.enc_hidden_size = 256\n",
        "        self.dec_hidden_size = 512 \n",
        "        self.Nz = 128\n",
        "        self.M = 20\n",
        "        self.rec_dropout = 0.1\n",
        "        self.batch_size = 100\n",
        "        self.eta_min = 0.01\n",
        "        self.R = 0.99995\n",
        "        self.KL_min = 0.2\n",
        "        self.wKL = 0.5\n",
        "        self.lr = 0.001\n",
        "        self.lr_decay = 0.9999\n",
        "        self.min_lr = 0.00001\n",
        "        self.grad_clip = 1.\n",
        "        self.temperature = 0.4\n",
        "        self.max_seq_length = 200\n",
        "        self.epochs = 100\n",
        "\n",
        "# just needed because it's not being imported\n",
        "HP = hyper_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MJkX94DbkO9"
      },
      "source": [
        "draw_strokes(test, svg_filename=\"cat.svg\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}